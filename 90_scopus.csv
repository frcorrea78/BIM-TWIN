Authors,Title,Year,Source title,Cited by,Link,Abstract,Document Type,Publication Stage,Source,EID
"Russakovsky O., Deng J., Su H., Krause J., Satheesh S., Ma S., Huang Z., Karpathy A., Khosla A., Bernstein M., Berg A.C., Fei-Fei L.","ImageNet Large Scale Visual Recognition Challenge",2015,"International Journal of Computer Vision",15762,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947041871&doi=10.1007%2fs11263-015-0816-y&partnerID=40&md5=ba4833102503cf8586d4f777f98ea99e","The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements. © 2015, Springer Science+Business Media New York.",Article,"Final",Scopus,2-s2.0-84947041871
"Ester M., Kriegel H.-P., Sander J., Xu X.","A density-based algorithm for discovering clusters in large spatial databases with noise",1996,"Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining",12047,,[No abstract available],,,Scopus,2-s2.0-0000550189
"Lloyd S.P.","Least Squares Quantization in PCM",1982,"IEEE Transactions on Information Theory",7886,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020102027&doi=10.1109%2fTIT.1982.1056489&partnerID=40&md5=93441bcaa6dfd4d4f33cec8fad6c4487","It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quantization schemes for 2b quanta, b = 1,2, ···, 7, are given numerically for Gaussian and for Laplacian distribution of signal amplitudes. ©1982 IEEE",Article,"Final",Scopus,2-s2.0-0020102027
"Qi C.R., Su H., Mo K., Guibas L.J.","PointNet: Deep learning on point sets for 3D classification and segmentation",2017,"Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017",3556,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043534104&doi=10.1109%2fCVPR.2017.16&partnerID=40&md5=e11897ee95a057bbc78aef23d420f926","Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption. © 2017 IEEE.",Conference Paper,"Final",Scopus,2-s2.0-85043534104
"Ankerst M., Breunig M.M., Kriegel H.-P., Sander J.","OPTICS: Ordering Points to Identify the Clustering Structure",1999,"SIGMOD Record (ACM Special Interest Group on Management of Data)",2656,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347172110&doi=10.1145%2f304181.304187&partnerID=40&md5=eceeb8ae651e60091b35f5b5c6b75200","Cluster analysis is a primary method for database mining. It is either used as a stand-alone tool to get insight into the distribution of a data set, e.g. to focus further analysis and data processing, or as a preprocessing step for other algorithms operating on the detected clusters. Almost all of the well-known clustering algorithms require input parameters which are hard to determine but have a significant influence on the clustering result. Furthermore, for many real-data sets there does not even exist a global parameter setting for which the result of the clustering algorithm describes the intrinsic clustering structure accurately. We introduce a new algorithm for the purpose of cluster analysis which does not produce a clustering of a data set explicitly; but instead creates an augmented ordering of the database representing its density-based clustering structure. This cluster-ordering contains information which is equivalent to the density-based clusterings corresponding to a broad range of parameter settings. It is a versatile basis for both automatic and interactive cluster analysis. We show how to automatically and efficiently extract not only 'traditional' clustering information (e.g. representative points, arbitrary shaped clusters), but also the intrinsic clustering structure. For medium sized data sets, the cluster-ordering can be represented graphically and for very large data sets, we introduce an appropriate visualization technique. Both are suitable for interactive exploration of the intrinsic clustering structure offering additional insights into the distribution and correlation of the data.",Article,"Final",Scopus,2-s2.0-0347172110
"Fisher R.A., Yates F.",[No title available],1963,"Statistical Tables for Biological, Agricultural and Medical Research",2404,,[No abstract available],,,Scopus,2-s2.0-0003741171
"Hoppe Hugues, DeRose Tony, Duchamp Tom, McDonald John, Stuetzle Werner","Surface reconstruction from unorganized points",1992,"Computer Graphics (ACM)",2068,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026888013&doi=10.1145%2f142920.134011&partnerID=40&md5=eb79072c6bfc5a01dd3221f17830486d","We describe and demonstrate an algorithm that takes as input an unorganized set of points {X1,...,nRTBC ⊂ IR3 on or near an unknown manifold M, and produces as output a simplicial surface that approximates M. Neither the topology, the presence of boundaries, nor the geometry of M are assumed to be known in advance - all are inferred automatically from the data. This problem naturally arises in a variety of practical situations such as range scanning an object from multiple view points, recovery of biological shapes from two-dimensional slices, and interactive surface sketching.",Article,"Final",Scopus,2-s2.0-0026888013
"Su H., Maji S., Kalogerakis E., Learned-Miller E.","Multi-view convolutional neural networks for 3D shape recognition",2015,"Proceedings of the IEEE International Conference on Computer Vision",1564,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973882748&doi=10.1109%2fICCV.2015.114&partnerID=40&md5=2d51348e0c4b74c3dd228cfb09a73e4a","A longstanding question in computer vision concerns the representation of 3D shapes for recognition: should 3D shapes be represented with descriptors operating on their native 3D formats, such as voxel grid or polygon mesh, or can they be effectively represented with view-based descriptors? We address this question in the context of learning to recognize 3D shapes from a collection of their rendered views on 2D images. We first present a standard CNN architecture trained to recognize the shapes' rendered views independently of each other, and show that a 3D shape can be recognized even from a single view at an accuracy far higher than using state-of-the-art 3D shape descriptors. Recognition rates further increase when multiple views of the shapes are provided. In addition, we present a novel CNN architecture that combines information from multiple views of a 3D shape into a single and compact shape descriptor offering even better recognition performance. The same architecture can be applied to accurately recognize human hand-drawn sketches of shapes. We conclude that a collection of 2D views can be highly informative for 3D shape recognition and is amenable to emerging CNN architectures and their derivatives. © 2015 IEEE.",Conference Paper,"Final",Scopus,2-s2.0-84973882748
"Lai K., Bo L., Ren X., Fox D.","A large-scale hierarchical multi-view RGB-D object dataset",2011,"Proceedings - IEEE International Conference on Robotics and Automation",1027,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84455168545&doi=10.1109%2fICRA.2011.5980382&partnerID=40&md5=996bca38baf12014f8c173d60f066088","Over the last decade, the availability of public image repositories and recognition benchmarks has enabled rapid progress in visual object category and instance detection. Today we are witnessing the birth of a new generation of sensing technologies capable of providing high quality synchronized videos of both color and depth, the RGB-D (Kinect-style) camera. With its advanced sensing capabilities and the potential for mass adoption, this technology represents an opportunity to dramatically increase robotic object recognition, manipulation, navigation, and interaction capabilities. In this paper, we introduce a large-scale, hierarchical multi-view object dataset collected using an RGB-D camera. The dataset contains 300 objects organized into 51 categories and has been made publicly available to the research community so as to enable rapid progress based on this promising technology. This paper describes the dataset collection procedure and introduces techniques for RGB-D based object recognition and detection, demonstrating that combining color and depth information substantially improves quality of results. © 2011 IEEE.",Conference Paper,"Final",Scopus,2-s2.0-84455168545
"Lee C.H., Varshney A., Jacobs D.W.","Mesh saliency",2005,"ACM Transactions on Graphics",600,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-24944488335&doi=10.1145%2f1073204.1073244&partnerID=40&md5=9f2bf8dcf949fb98daee2210ba1bfce7","Research over the last decade has built a solid mathematical foundation for representation and analysis of 3D meshes in graphics and geometric modeling. Much of this work however does not explicitly incorporate models of low-level human visual attention. In this paper we introduce the idea of mesh saliency as a measure of regional importance for graphics meshes. Our notion of saliency is inspired by low-level human visual system cues. We define mesh saliency in a scale-dependent manner using a center-surround operator on Gaussian-weighted mean curvatures. We observe that such a definition of mesh saliency is able to capture what most would classify as visually interesting regions on a mesh. The human-perception-inspired importance measure computed by our mesh saliency operator results in more visually pleasing results in processing and viewing of 3D meshes, compared to using a purely geometric measure of shape, such as curvature. We discuss how mesh saliency can be incorporated in graphics applications such as mesh simplification and viewpoint selection and present examples that show visually appealing results from using mesh saliency. Copyright © 2005 by the Association for Computing Machinery, Inc.",Conference Paper,"Final",Scopus,2-s2.0-24944488335
"Armeni I., Sener O., Zamir A.R., Jiang H., Brilakis I., Fischer M., Savarese S.","3D semantic parsing of large-scale indoor spaces",2016,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition",508,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986325920&doi=10.1109%2fCVPR.2016.170&partnerID=40&md5=e9ab45f07a81a40d4de3ff7333ee85c8","In this paper, we propose a method for semantic parsing the 3D point cloud of an entire building using a hierarchical approach: first, the raw data is parsed into semantically meaningful spaces (e.g. rooms, etc) that are aligned into a canonical reference coordinate system. Second, the spaces are parsed into their structural and building elements (e.g. walls, columns, etc). Performing these with a strong notation of global 3D space is the backbone of our method. The alignment in the first step injects strong 3D priors from the canonical coordinate system into the second step for discovering elements. This allows diverse challenging scenarios as man-made indoor spaces often show recurrent geometric patterns while the appearance features can change drastically. We also argue that identification of structural elements in indoor spaces is essentially a detection problem, rather than segmentation which is commonly used. We evaluated our method on a new dataset of several buildings with a covered area of over 6, 000m2 and over 215 million points, demonstrating robust results readily useful for practical applications. © 2016 IEEE.",Conference Paper,"Final",Scopus,2-s2.0-84986325920
"Wang P.-S., Liu Y., Guo Y.-X., Sun C.-Y., Tong X.","O-CNN: Octree-based convolutional neural networks for 3D shape analysis",2017,"ACM Transactions on Graphics",471,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030781395&doi=10.1145%2f3072959.3073608&partnerID=40&md5=3f897f2484a05d94d2a5d74385ab8353","We present O-CNN, an Octree-based Convolutional Neural Network (CNN) for 3D shape analysis. Built upon the octree representation of 3D shapes, our method takes the average normal vectors of a 3D model sampled in the finest leaf octants as input and performs 3D CNN operations on the octants occupied by the 3D shape surface. We design a novel octree data structure to efficiently store the octant information and CNN features into the graphics memory and execute the entire O-CNN training and evaluation on the GPU. O-CNN supports various CNN structures and works for 3D shapes in different representations. By restraining the computations on the octants occupied by 3D surfaces, the memory and computational costs of the O-CNN grow quadratically as the depth of the octree increases, which makes the 3D CNN feasible for high-resolution 3D models. We compare the performance of the O-CNN with other existing 3D CNN solutions and demonstrate the efficiency and efficacy of O-CNN in three shape analysis tasks, including object classification, shape retrieval, and shape segmentation. © 2017 Association for Computing Machinery.",Conference Paper,"Final",Scopus,2-s2.0-85030781395
"Vázquez P.-P., Feixas M., Sbert M., Heidrich W.","Viewpoint selection using viewpoint entropy",2001,"Proceedings of the Vision Modeling and Visualization Conference (VMV-01)",284,,[No abstract available],,"Final",Scopus,2-s2.0-1642490827
"Hackel T., Savinov N., Ladicky L., Wegner J.D., Schindler K., Pollefeys M.","SEMANTIC3D.NET: A NEW LARGE-SCALE POINT CLOUD CLASSIFICATION BENCHMARK",2017,"ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences",240,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040692236&doi=10.5194%2fisprs-annals-IV-1-W1-91-2017&partnerID=40&md5=fd39b83943b97e48d15923780fea366d","This paper presents a new 3D point cloud classification benchmark data set with over four billion manually labelled points, meant as input for data-hungry (deep) learning methods. We also discuss first submissions to the benchmark that use deep convolutional neural networks (CNNs) as a work horse, which already show remarkable performance improvements over state-of-the-art. CNNs have become the de-facto standard for many tasks in computer vision and machine learning like semantic segmentation or object detection in images, but have no yet led to a true breakthrough for 3D point cloud labelling tasks due to lack of training data. With the massive data set presented in this paper, we aim at closing this data gap to help unleash the full potential of deep learning methods for 3D labelling tasks. Our semantic3D.net data set consists of dense point clouds acquired with static terrestrial laser scanners. It contains 8 semantic classes and covers a wide range of urban outdoor scenes: churches, streets, railroad tracks, squares, villages, soccer fields and castles. We describe our labelling interface and show that our data set provides more dense and complete point clouds with much higher overall number of labelled points compared to those already available to the research community. We further provide baseline method descriptions and comparison between methods submitted to our online system. We hope semantic3D.net will pave the way for deep learning methods in 3D point cloud labelling to learn richer, more general 3D representations, and first submissions after only a few months indicate that this might indeed be the case. © 2017 Copernicus GmbH. All rights reserved.",Conference Paper,"Final",Scopus,2-s2.0-85040692236
"Mitra N.J., Nguyen A.","Estimating surface normals in noisy point cloud data",2003,"Proceedings of the Annual Symposium on Computational Geometry",239,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038376469&doi=10.1145%2f777792.777840&partnerID=40&md5=e2caa1649c7cf261d998339db040d3d9","In this paper we describe and analyze a method based on local least square fitting for estimating the normals at all sample points of a point cloud data (PCD) set, in the presence of noise. We study the effects of neighborhood size, curvature, sampling density, and noise on the normal estimation when the PCD is sampled from a smooth curve in ℝ2 or a smooth surface in ℝ3 and noise is added. The analysis allows us to find the optimal neighborhood size using other local information from the PCD. Experimental results are also provided.",Conference Paper,"Final",Scopus,2-s2.0-0038376469
"Armeni I., Sax A., Zamir A.R., Savarese S.","Joint 2D-3D-Semantic Data for Indoor Scene Understanding",2017,"Joint 2D-3D-Semantic Data for Indoor Scene Understanding",232,,[No abstract available],,"Final",Scopus,2-s2.0-85030773563
"Kanezaki A., Matsushita Y., Nishida Y.","RotationNet: Joint Object Categorization and Pose Estimation Using Multiviews from Unsupervised Viewpoints",2018,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition",198,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055492477&doi=10.1109%2fCVPR.2018.00526&partnerID=40&md5=7741c20edffa72919ca0c7a5c44768ba","We propose a Convolutional Neural Network (CNN)-based model 'RotationNet,' which takes multi-view images of an object as input and jointly estimates its pose and object category. Unlike previous approaches that use known viewpoint labels for training, our method treats the viewpoint labels as latent variables, which are learned in an unsupervised manner during the training using an unaligned object dataset. RotationNet is designed to use only a partial set of multi-view images for inference, and this property makes it useful in practical scenarios where only partial views are available. Moreover, our pose alignment strategy enables one to obtain view-specific feature representations shared across classes, which is important to maintain high accuracy in both object categorization and pose estimation. Effectiveness of RotationNet is demonstrated by its superior performance to the state-of-the-art methods of 3D object classification on 10- and 40-class ModelNet datasets. We also show that RotationNet, even trained without known poses, achieves the state-of-the-art performance on an object pose estimation dataset. © 2018 IEEE.",Conference Paper,"Final",Scopus,2-s2.0-85055492477
"Blanz V., Tarr M.J., Bülthoff H.H.","What object attributes determine canonical views?",1999,"Perception",191,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033253934&doi=10.1068%2fp2897&partnerID=40&md5=33765525b4689d846266defcb8ee2492","We investigated preferred or canonical views for familiar and three-dimensional non-sense objects using computer-graphics psychophysics. We assessed the canonical views for objects by allowing participants to actively rotate realistically shaded three-dimensional models in real-time. Objects were viewed on a Silicon Graphics workstation and manipulated in virtual space with a three-degree-of-freedom input device. In the first experiment, participants adjusted each object to the viewpoint from which they would take a photograph if they planned to use the object to illustrate a brochure. In the second experiment, participants mentally imaged each object on the basis of the name and then adjusted the object to the viewpoint from which they imagined it. In both experiments, there was a large degree of consistency across participants in terms of the preferred view for a given object. Our results provide new insights on the geometrical, experiential, and functional attributes that determine canonical views under ecological conditions.",Article,"Final",Scopus,2-s2.0-0033253934
"Jing H., You S.","Point cloud labeling using 3D Convolutional Neural Network",2016,"Proceedings - International Conference on Pattern Recognition",166,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019157984&doi=10.1109%2fICPR.2016.7900038&partnerID=40&md5=e86e97918c8c33e037f162a77bba48bd","In this paper, we tackle the labeling problem for 3D point clouds. We introduce a 3D point cloud labeling scheme based on 3D Convolutional Neural Network. Our approach minimizes the prior knowledge of the labeling problem and does not require a segmentation step or hand-crafted features as most previous approaches did. Particularly, we present solutions for large data handling during the training and testing process. Experiments performed on the urban point cloud dataset containing 7 categories of objects show the robustness of our approach. © 2016 IEEE.",Conference Paper,"Final",Scopus,2-s2.0-85019157984
"Ioannidou A., Chatzilari E., Nikolopoulos S., Kompatsiaris I.","Deep learning advances in computer vision with 3D data: A survey",2017,"ACM Computing Surveys",153,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017427136&doi=10.1145%2f3042064&partnerID=40&md5=55bbd9fb3fa1f592c25ec1f0b86859ac","Deep learning has recently gained popularity achieving state-of-the-art performance in tasks involving text, sound, or image processing. Due to its outstanding performance, there have been efforts to apply it in more challenging scenarios, for example, 3D data processing. This article surveys methods applying deep learning on 3D data and provides a classification based on how they exploit them. From the results of the examined works, we conclude that systems employing 2D views of 3D data typically surpass voxel-based (3D) deep models, which however, can perform better with more layers and severe data augmentation. Therefore, larger-scale datasets and increased resolutions are required. © 2017 ACM.",Review,"Final",Scopus,2-s2.0-85017427136
"Teicholz P.","BIM for facility managers",2018,"BIM for Facility Managers",138,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103040849&doi=10.1002%2f9781119572633&partnerID=40&md5=038ef8b45bb5d3a9e92de90bb9b483cc","A practical look at extending the value of Building Information Modeling (BIM) into facility management-from the world’s largest international association for professional facility managers Building owners and facility managers are discovering that Building Information Modeling (BIM) models of buildings are deep reservoirs of information that can provide valuable spatial and mechanical details on every aspect of a property. When used appropriately, this data can improve performance and save time, effort, and money in running and maintaining the building during its life cycle. It can also provide information for future modifications. For instance, a BIM could reveal everything from the manufacturer of a light fixture to its energy usage to maintenance instructions. BIM for Facility Managers explains how BIM can be linked to facility management (FM) systems to achieve very significant life-cycle advantages. It presents guidelines for using BIM in FM that have been developed by public and private owners such as the GSA. There is an extensive discussion of the legal and contractual issues involved in BIM/FM integration. It describes how COBie can be used to name, capture, and communicate FM-related data to downstream systems. There is also extensive discussion of commercial software tools that can be used to facilitate this integration. This book features six in-depth case studies that illustrate how BIM has been successfully integrated with facility management in real-life projects at: • Texas A&M Health Science Center • USC School of Cinematic Arts • MathWork’s new campus • Xavier University • State of Wisconsin Facilities • University of Chicago Library renovation BIM for Facility Managers is an indispensable resource for facility managers, building owners, and developers alike. © 2013 by John Wiley & Sons, Inc.",Book,"Final",Scopus,2-s2.0-85103040849
"Takahashi S., Fujishiro I., Takeshima Y., Nishita T.","A feature-driven approach to locating optimal viewpoints for volume visualization",2005,"Proceedings of the IEEE Visualization Conference",124,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749452031&doi=10.1109%2fVIS.2005.4&partnerID=40&md5=cf1d5d5c20e63f341264b8b40889e391","Optimal viewpoint selection is an important task because it considerably influences the amount of information contained in the 2D projected images of 3D objects, and thus dominates their first impressions from a psychological point of view. Although several methods have been proposed that calculate the optimal positions of viewpoints especially for 3D surface meshes, none has been done for solid objects such as volumes. This paper presents a new method of locating such optimal viewpoints when visualizing volumes using direct volume rendering. The major idea behind our method is to decompose an entire volume into a set of feature components, and then find a globally optimal viewpoint by finding a compromise between locally optimal viewpoints for the components. As the feature components, the method employs interval volumes and their combinations that characterize the topological transitions of isosurfaces according to the scalar field. Furthermore, opacity transfer functions are also utilized to assign different weights to the decomposed components so that users can emphasize features of specific interest in the volumes. Several examples of volume datasets together with their optimal positions of view-points are exhibited in order to demonstrate that the method can effectively guide naive users to find optimal projections of volumes. © 2005 IEEE.",Conference Paper,"Final",Scopus,2-s2.0-33749452031
"Girardeau-Montaut D.","CloudCompare-Open Source Project, Open Source Project",2011,"Cloudcompare-Open Source Project",113,,[No abstract available],,"Final",Scopus,2-s2.0-84943572790
"Wang C., Pelillo M., Siddiqi K.","Dominant set clustering and pooling for multi-view 3D object recognition",2017,"British Machine Vision Conference 2017, BMVC 2017",72,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074950786&doi=10.5244%2fc.31.64&partnerID=40&md5=410adb980b80819b437e315a3d5045e5","View based strategies for 3D object recognition have proven to be very successful. The state-of-the-art methods now achieve over 90% correct category level recognition performance on appearance images. We improve upon these methods by introducing a view clustering and pooling layer based on dominant sets. The key idea is to pool information from views which are similar and thus belong to the same cluster. The pooled feature vectors are then fed as inputs to the same layer, in a recurrent fashion. This recurrent clustering and pooling module, when inserted in an off-the-shelf pretrained CNN, boosts performance for multi-view 3D object recognition, achieving a new state of the art test set recognition accuracy of 93.8% on the ModelNet 40 database. We also explore a fast approximate learning strategy for our cluster-pooling CNN, which, while sacrificing end-to-end learning, greatly improves its training efficiency with only a slight reduction of recognition accuracy to 93.3%. Our implementation is available at https://github.com/fate3439/dscnn. © 2017. The copyright of this document resides with its authors.",Conference Paper,"Final",Scopus,2-s2.0-85074950786
"Yamauchi H., Saleem W., Yoshizawa S., Karni Z., Belyaev A., Seidel H.-P.","Towards stable and salient multi-view representation of 3D shapes",2006,"Proceedings - IEEE International Conference on Shape Modeling and Applications 2006, SMI 2006",67,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845437122&doi=10.1109%2fSMI.2006.42&partnerID=40&md5=5809b0dee1da69a397d23660c2277202","An approach to automatically select stable and salient representative views of a given 3D object is proposed. Initially, a set of viewpoints are uniformly sampled along the surface of a bounding sphere. The sampled viewpoints are connected to their closest points to form a spherical graph in which each edge is weighted by a similarity measure between the two views from its incident vertices. Partitions of similar views are obtained using a graph partitioning procedure and their ""centroids"" are considered to be their representative views. Finally, the views are ranked based on a saliency measure to form the object's representative views. This leads to a compact, human-oriented 2D description of a 3D object, and as such, is useful both for traditional applications like presentation and analysis of 3D shapes, and for emerging ones like indexing and retrieval in large shape repositories. © 2006 IEEE.",Conference Paper,"Final",Scopus,2-s2.0-33845437122
"You H., Ji R., Feng Y., Gao Y.","PVNet: A joint convolutional network of point cloud and multi-view for 3D shape recognition",2018,"MM 2018 - Proceedings of the 2018 ACM Multimedia Conference",62,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058228992&doi=10.1145%2f3240508.3240702&partnerID=40&md5=8778a6d034852a509f530de96f9050e9","3D object recognition has attracted wide research attention in the field of multimedia and computer vision. With the recent proliferation of deep learning, various deep models with different representations have achieved the state-of-the-art performance. Among them, point cloud and multi-view based 3D shape representations are promising recently, and their corresponding deep models have shown significant performance on 3D shape recognition. However, there is little effort concentrating point cloud data and multi-view data for 3D shape representation, which is, in our consideration, beneficial and compensated to each other. In this paper, we propose the Point-View Network (PVNet), the first framework integrating both the point cloud and the multi-view data towards joint 3D shape recognition. More specifically, an embedding attention fusion scheme is proposed that could employ high-level features from the multi-view data to model the intrinsic correlation and discrim-inability of different structure features from the point cloud data. In particular, the discriminative descriptions are quantified and leveraged as the soft attention mask to further refine the structure feature of the 3D shape. We have evaluated the proposed method on the ModelNet40 dataset for 3D shape classification and retrieval tasks. Experimental results and comparisons with state-of-the-art methods demonstrate that our framework can achieve superior performance. © 2018 Association for Computing Machinery.",Conference Paper,"Final",Scopus,2-s2.0-85058228992
"Dutagaci H., Cheung C.P., Godil A.","A benchmark for best view selection of 3D objects",2010,"3DOR'10 - Proceedings of the 2010 ACM Workshop on 3D Object Retrieval, Co-located with ACM Multimedia 2010",50,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650442628&doi=10.1145%2f1877808.1877819&partnerID=40&md5=49ce1d42dcbb849957658baaf3328df4","The best view selection corresponds to the task of automatically selecting the most representative view of a 3D model. In this paper, we describe a benchmark for evaluation of best view selection algorithms. The benchmark consists of the preferred views of 68 3D models provided by 26 human subjects. The data was collected using a web-based subjective experiment where the users were asked to select the most informative view of a 3D model. We provided a quantitative evaluation measure based on this ground truth data, and compared the performances of seven best view selection algorithms.",Conference Paper,"Final",Scopus,2-s2.0-78650442628
"Kim D.I., Sukhatme G.S.","Semantic labeling of 3D point clouds with object affordance for robot manipulation",2014,"Proceedings - IEEE International Conference on Robotics and Automation",33,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929223213&doi=10.1109%2fICRA.2014.6907679&partnerID=40&md5=b65dee30f53b3b8ac896c048f607c0ca","When a robot is deployed it needs To understand The nature of its surroundings. In This paper, we address The problem of semantic labeling 3D point clouds by object affordance (e.g., 'pushable', 'liftable'). We propose a Technique To extract geometric features from point cloud segments and build a classifier To predict associated object affordances. With The classifier, we have developed an algorithm To enhance object segmentation and reduce manipulation uncertainty by iterative clustering, along with minimizing labeling entropy. Our incremental multiple view merging Technique shows improved object segmentation. The novel feature of our approach is The semantic labeling That can be directly applied To manipulation planning. In our experiments with 6 affordance labels, an average of 81.8% accuracy of affordance prediction is achieved. We demonstrate refined object segmentation by applying The classifier To data from The PR2 robot using a Microsoft Kinect in an indoor office environment. © 2014 IEEE.",Conference Paper,"Final",Scopus,2-s2.0-84929223213
"Hagedorn B., Döllner J.","High-level web service for 3D building information visualization and analysis",2007,"GIS: Proceedings of the ACM International Symposium on Advances in Geographic Information Systems",30,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959674344&doi=10.1145%2f1341012.1341023&partnerID=40&md5=e406b3f83224ceb820196b129a53151a","This paper presents an approach to visualize and analyze 3D building information models within virtual 3D city models. Building information models (BIMs) formalize and represent detailed information related to the lifecycle of buildings, e.g., information about composition, facilities, equipment, usage, maintenance, and workflows such as rescue scenarios. Complementary, virtual 3D city models represent objects and phenomena of urban areas, typically at a lower level of information detail; virtual 3D city models as a general framework and platform for spatial data allow us to seamlessly combine GIS and BIM data. In our approach, BIM data and 3D geodata, both provided by possibly distributed, heterogeneous web services, are efficiently integrated by the underlying real-time 3D geovisualization system. To facilitate insights into complex spatial scenarios, two configurable BIM-specific visualization techniques have been developed, which map BIM data onto 3D building graphics variables respectively geometrically distort 3D building representations. The visualization functionality can itself be accessed as a specialized web 3D perspective view service. We demonstrate our approach by a fire and rescue scenario for a part of a 3D campus model. © 2007 ACM.",Conference Paper,"Final",Scopus,2-s2.0-79959674344
"Aljumaily H., Laefer D.F., Cuadra D.","Urban Point Cloud Mining Based on Density Clustering and MapReduce",2017,"Journal of Computing in Civil Engineering",28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032210646&doi=10.1061%2f%28ASCE%29CP.1943-5487.0000674&partnerID=40&md5=cd4fa6dc51ec0fccc13d18c7a94aeb39","This paper proposes an approach to classify, localize, and extract automatically urban objects such as buildings and the ground surface from a digital surface model created from aerial laser scanning data. To achieve that, the approach involves three steps: (1) dividing the original data into smaller, more manageable pieces using a method based on MapReduce gridding for subspace partitioning, (2) applying the DBSCAN algorithm to identify interesting subspaces depending on point density, and (3) grouping of identified subspaces to form potential objects. Validation of the method was conducted in an architecturally dense and complex portion of Dublin, Ireland. The best results were achieved with a 1-m3-sized clustering cube, for which the number of classified clusters most closely equaled that which was derived manually (correctness=84.91%, completeness=84.39%, and quality=84.65%). © 2017 American Society of Civil Engineers.",Article,"Final",Scopus,2-s2.0-85032210646
"Cabello R.",[No title available],2010,"Three.JS",27,,[No abstract available],,"Final",Scopus,2-s2.0-84865546362
"Riemenschneider H., Bodis-Szomoru A., Weissenberg J., Van Gool L.","Learning where to classify in multi-view semantic segmentation",2014,"ECCV",25,,[No abstract available],,"Final",Scopus,2-s2.0-84925330387
"Stojanovic V., Trapp M., Richter R., Hagedorn B., Döllner J.","Towards the generation of digital twins for facility management based on 3D point clouds",2018,"Proceeding of the 34th Annual ARCOM Conference, ARCOM 2018",20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054529484&partnerID=40&md5=362878d63e18bee90afc8761c9203163","Advances versus adaptation of Industry 4.0 practices in Facility Management (FM) have created usage demand for up-to-date digitized building assets. The use of Building Information Modelling (BIM) for FM in the Operation and Maintenance (O&M) stages of the building lifecycle is intended to bridge the gap between operations and digital data, but lacks the functionality of assessing and forecasting the state of the built environment in real-time. To accommodate this, BIM data needs to be constantly updated with the current state of the built environment. However, generation of as-is BIM data for a digital representation of a building is a labor intensive process. While some software applications offer a degree of automation for the generation of as-is BIM data, they can be impractical to use for routinely updating digital FM documentation. Current approaches for capturing the built environment using remote sensing and photometry-based methods allow for the creation of 3D point clouds that can be used as basis data for a Digital Twin (DT), along with existing BIM and FM documentation. 3D point clouds themselves do not contain any semantics or specific information about the building components they represent physically, but using machine learning methods they can be enhanced with semantics that would allow for reconstruction of as-is BIM and basis DT data. This paper presents current research and development progress of a service-oriented platform for generation of semantically rich 3D point cloud representations of indoor environments. A specific focus is placed on the reconstruction and visualization of the captured state of the built environment for increasing FM stakeholder engagement and facilitating collaboration. The preliminary results of a prototypical web-based application demonstrate the feasibility of such a platform for FM using a service-oriented paradigm. © Proceeding of the 34th Annual ARCOM Conference, ARCOM 2018.",Conference Paper,"Final",Scopus,2-s2.0-85054529484
"Discher S., Richter R., Döllner J.","A scalable WebGL-based approach for visualizing massive 3D point clouds using semantics-Dependent rendering techniques",2018,"Proceedings - Web3D 2018: 23rd International ACM Conference on 3D Web Technology",14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050612018&doi=10.1145%2f3208806.3208816&partnerID=40&md5=13601d7b9157fbf1af29c4c92ed6d02f","3D point cloud technology facilitates the automated and highly detailed digital acquisition of real-world environments such as assets, sites, cities, and countries; the acquired 3D point clouds represent an essential category of geodata used in a variety of geoinformation applications and systems. In this paper, we present a web-based system for the interactive and collaborative exploration and inspection of arbitrary large 3D point clouds. Our approach is based on standard WebGL on the client side and is able to render 3D point clouds with billions of points. It uses spatial data structures and level-of-detail representations to manage the 3D point cloud data and to deploy out-of-core and web-based rendering concepts. By providing functionality for both, thin-client and thick-client applications, the system scales for client devices that are vastly different in computing capabilities. Different 3D point-based rendering techniques and post-processing effects are provided to enable task-specific and data-specific filtering and highlighting, e.g., based on per-point surface categories or temporal information. A set of interaction techniques allows users to collaboratively work with the data, e.g., by measuring distances and areas, by annotating, or by selecting and extracting data subsets. Additional value is provided by the system’s ability to display additional, context-providing geodata alongside 3D point clouds and to integrate task-specific processing and analysis operations. We have evaluated the presented techniques and the prototype system with different data sets from aerial, mobile, and terrestrial acquisition campaigns with up to 120 billion points to show their practicality and feasibility. © 2018 Association for Computing Machinery.",Conference Paper,"Final",Scopus,2-s2.0-85050612018
"Bonaventura X., Feixas M., Sbert M., Chuang L., Wallraven C.","A survey of viewpoint selection methods for polygonal models",2018,"Entropy",14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053888747&doi=10.3390%2fe20050370&partnerID=40&md5=584a75628ecdd2cb6717d275b380a7d7","Viewpoint selection has been an emerging area in computer graphics for some years, and it is now getting maturity with applications in fields such as scene navigation, scientific visualization, object recognition, mesh simplification, and camera placement. In this survey, we review and compare twenty-two measures to select good views of a polygonal 3D model, classify them using an extension of the categories defined by Secord et al., and evaluate them against the Dutagaci et al. benchmark. Eleven of these measures have not been reviewed in previous surveys. Three out of the five short-listed best viewpoint measures are directly related to information. We also present in which fields the different viewpoint measures have been applied. Finally, we provide a publicly available framework where all the viewpoint selection measures are implemented and can be compared against each other. © 2018 by the authors.",Article,"Final",Scopus,2-s2.0-85053888747
"Mura C., Wyss G., Pajarola R.","Robust normal estimation in unstructured 3D point clouds by selective normal space exploration",2018,"Visual Computer",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046540473&doi=10.1007%2fs00371-018-1542-6&partnerID=40&md5=5cfd5b2935e6ae9fd5e4ec6502d92f74","We present a fast and practical approach for estimating robust normal vectors in unorganized point clouds. Our proposed technique is robust to noise and outliers and can preserve sharp features in the input model while being significantly faster than the current state-of-the-art alternatives. The key idea to this is a novel strategy for the exploration of the normal space: First, an initial candidate normal vector, optimal under a robust least median norm, is selected from a discrete subregion of this space, chosen conservatively to include the correct normal; then, the final robust normal is computed, using a simple, robust procedure that iteratively refines the candidate normal initially selected. This strategy allows us to reduce the computation time significantly with respect to other methods based on sampling consensus and yet produces very reliable normals even in the presence of noise and outliers as well as along sharp features. The validity of our approach is confirmed by an extensive testing on both synthetic and real-world data and by a comparison against the most relevant state-of-the-art approaches. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.",Article,"Final",Scopus,2-s2.0-85046540473
"Castelló P., Sbert M., Chover M., Feixas M.","Techniques for computing viewpoint entropy of a 3D scene",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746628487&doi=10.1007%2f11758525_35&partnerID=40&md5=2ffa67b7a3d4582e609a49093b879202","Viewpoint entropy is a metric that allows measuring the visibility goodness of a scene from a camera position. In this work, we analyze different software and hardware assisted techniques to compute the viewpoint entropy. The main objective of this study is to identify which of these techniques can be used in real time for 3D scenes of high complexity. Our results show that interactivity can be obtained with occlusion query technique and that for real time we need a hybrid software and hardware technique. © Springer-Verlag Berlin Heidelberg 2006.",Conference Paper,"Final",Scopus,2-s2.0-33746628487
"Stojanovic V., Trapp M., Richter R., Döllner J.","A service-Oriented approach for classifying 3D points clouds by example of office furniture classification",2018,"Proceedings - Web3D 2018: 23rd International ACM Conference on 3D Web Technology",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050601423&doi=10.1145%2f3208806.3208810&partnerID=40&md5=1fa7f0545b1036e7cc7bbb408600bc20","The rapid digitalization of the Facility Management (FM) sector has increased the demand for mobile, interactive analytics approaches concerning the operational state of a building. These approaches provide the key to increasing stakeholder engagement associated with Operation and Maintenance (O&M) procedures of living and working areas, buildings, and other built environment spaces. We present a generic and fast approach to process and analyze given 3D point clouds of typical indoor office spaces to create corresponding up-to-date approximations of classified segments and object-based 3D models that can be used to analyze, record and highlight changes of spatial configurations. The approach is based on machine-learning methods used to classify the scanned 3D point cloud data using 2D images. This approach can be used to primarily track changes of objects over time for comparison, allowing for routine classification, and presentation of results used for decision making. We specifically focus on classification, segmentation, and reconstruction of multiple different object types in a 3D point-cloud scene. We present our current research and describe the implementation of these technologies as a web-based application using a services-oriented methodology. © 2018 Association for Computing Machinery.",Conference Paper,"Final",Scopus,2-s2.0-85050601423
"Dietze A., Klomann M., Jung Y., Englert M., Rieger S., Rehberger A., Hau S., Grimm P.","SMULGRAS: A platform for smart multicodal graphics search",2017,"Proceedings - Web3D 2017: 22nd International Conference on 3D Web Technology",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021797208&doi=10.1145%2f3055624.3075942&partnerID=40&md5=92f824b565c9b3fcb07c1d41a042604d","In this paper we describe our SMULGRAS platform for smart multicodal graphics search, which aims at fusing web-based content creation tools and content-based search interfaces. Our framework provides an easy to use web frontend that integrates a 3D editor capable of creating, editing, and presenting 3D scenes as well as intuitive interfaces for image/graphics search, which can serve queries by both, 3D models or camera-captured footage. Correspondingly, our proposed backend pipeline allows a similarity search both by image / shape and for images / shapes. To do so, we employ state-of-The-Art deep learning techniques based on convolutional neural networks, which employ to both, shape and image representations. Our framework not only provides good retrieval accuracy as well as scalability with training and retrieval, but it also gives the user more control over the (iterative) search process by directly integrating fully interactive, web-based editing tools. .is makes the approach also suitable for usage within large-scale community-based modeling applications. © 2017 ACM.",Conference Paper,"Final",Scopus,2-s2.0-85021797208
"Wolf J., Richter R., Döllner J.","Techniques for automated classification and segregation of mobile mapping 3D point clouds",2019,"VISIGRAPP 2019 - Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068208223&doi=10.5220%2f0007308802010208&partnerID=40&md5=e1b22c6a59fa513e7a33f06be98273b3","We present an approach for the automated classification and segregation of initially unordered and unstructured large 3D point clouds from mobile mapping scans. It derives disjoint point sub-clouds belonging to general surface categories such as ground, building, and vegetation. It provides a semantics-based classification by identifying typical assets in road-like environments such as vehicles and post-like structures, e. g., road signs or lamps, which are relevant for many applications using mobile scans. We present an innovative processing pipeline that allows for a semantic class detection for all points of a 3D point cloud in an automated process based solely on topology information. Our approach uses adaptive segmentation techniques as well as characteristic per-point attributes of the surface and the local point neighborhood. The techniques can be efficiently implemented and can handle large city-wide scans with billions of points, while still being easily adaptable to specific application domains and needs. The techniques can be used as base functional components in applications and systems for, e. g., asset detection, road inspection, cadastre validation, and support the automation of corresponding tasks. We have evaluated our techniques in a prototypical implementation on three datasets with different characteristics and show their practicability for these representative use cases. Copyright © 2019 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved",Conference Paper,"Final",Scopus,2-s2.0-85068208223
"Qu T., Coco J., Rönnäng M., Sun W.","Challenges and trends of implementation of 3D point cloud technologies in building information modeling (BIM): Case studies",2014,"Computing in Civil and Building Engineering - Proceedings of the 2014 International Conference on Computing in Civil and Building Engineering",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934325453&doi=10.1061%2f9780784413616.101&partnerID=40&md5=7d5f8cff685893fa70e1d3a5e09b79bb","Building Information Modeling (BIM) has gained wider acceptance in the Architecture/Engineering/Construction (A/E/C) industries in the U.S. and internationally. Capability of BIM technologies in early detection of spatial conflicts among various building systems and components concurrently designed by different design team members prior to construction has great potential in reducing construction delays, loss of productivity, and overall construction costs. Recent advancements in 3D point cloud technologies, such as 3D laser scanning, can greatly augment BIM technology applications in the A/E/C industry by providing a rapid three-dimensional capture of the built environment in great detail and accuracy. This paper discusses the current approaches of implementing 3D point cloud data in BIM and virtual design and construction (VDC) applications during various stages of a project lifecycle and the challenges associated with processing the huge amount of 3D point cloud data. Conversion from discrete 3D point cloud raster data to geometric/vector BIM data remains to be a labor-intensive process. The needs for intelligent geometric feature detection/reconstruction algorithms for automated point cloud processing and issues related to data management will be discussed. This paper also presents some innovative approaches for integrating 3D point cloud data with BIM to efficiently augment built environment design, construction, and management. © ASCE 2014.",Conference Paper,"Final",Scopus,2-s2.0-84934325453
"Discher S., Richter R., Trapp M., Döllner J.","Service-oriented processing and analysis of massive point clouds in geoinformation management",2018,"Service Oriented Mapping: Changing Paradigm in Map Production and Geoinformation Management",3,,[No abstract available],,"Final",Scopus,2-s2.0-85050615110
"Runceanu L.S., Haala N.","Indoor mesh classification for bim",2018,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056168081&doi=10.5194%2fisprs-archives-XLII-4-535-2018&partnerID=40&md5=502f9a50e425a283823c1181617e6c02","This work addresses the automatic reconstruction of objects useful for BIM, like walls, floors and ceilings, from meshed and textured mapped 3D point clouds of indoor scenes. For this reason, we focus on the semantic segmentation of 3D indoor meshes as the initial step for the automatic generation of BIM models. Our investigations are based on the benchmark dataset ScanNet, which aims at the interpretation of 3D indoor scenes. For this purpose it provides 3D meshed representations as collected from low cost range cameras. In our opinion such RGB-D data has a great potential for the automated reconstruction of BIM objects. © Authors 2018.",Conference Paper,"Final",Scopus,2-s2.0-85056168081
"Huang X., Wang M., Zhang D., Zhu Y., Zou L., Sun J., Han F., He L.","Multi-view Fusion with Deep Learning for 3D Shape Classification",2018,"ICALIP 2018 - 6th International Conference on Audio, Language and Image Processing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063482923&doi=10.1109%2fICALIP.2018.8455827&partnerID=40&md5=3a00481bcbc1d8e5ffd01c3e2713883c","3D model retrieval is all along a difficult and hotspot in computer vision. Recently, the view-based methods make use of the advanced convolutional neural networks which achieve excellent results. However, the structural information of the 3D model was destroyed by the projection and the relevance of multiple perspectives was not considered. In order to resolve this problem, this manuscript analyzes the process of human observation of 3D models and imitates the process of human recognition of 3D models through a combination of convolutional neural networks and recurrent neural networks. Our approach can convert the relevance between perspectives into the dependency relationship between structured recurrent neural networks and restore the structural information of the 3D model. We conducted experiments on the ModelNet40 database, it shows strong performance on par or even better than state-of-The-Art results. © 2018 IEEE.",Conference Paper,"Final",Scopus,2-s2.0-85063482923
"Zhu Y., Shepstone S.E., Martínez-Nuevo P., Kristoffersen M.S., Moutarde F., Fu Z.","Multiview based 3d scene understanding on partial point sets",2018,"CoRR",1,,[No abstract available],,"Final",Scopus,2-s2.0-85071167789
"Lauri M., Atanasov N., Pappas G., Ritala R.","Active object recognition via Monte Carlo tree search",2015,"Workshop on beyond Geometric Constraints at the International Conference on Robotics and Automation (ICRA)",1,,[No abstract available],,"Final",Scopus,2-s2.0-85071163165
