Authors,Title,Year,Source title,Cited by,Link,Abstract,Document Type,Publication Stage,Source,EID
"Alpaydin E.",[No title available],2004,"Introduction to Machine Learning",4735,,[No abstract available],,,Scopus,2-s2.0-25644459607
"Wolpert D.H.","Stacked generalization",1992,"Neural Networks",3802,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026692226&doi=10.1016%2fS0893-6080%2805%2980023-1&partnerID=40&md5=1b507c100ad8744db25ea83249c00f6e","This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory. © 1992 Pergamon Press Ltd.",Article,"Final",Scopus,2-s2.0-0026692226
"Ke G., Meng Q., Finley T., Wang T., Chen W., Ma W., Ye Q., Liu T.-Y.","LightGBM: A highly efficient gradient boosting decision tree",2017,"Advances in Neural Information Processing Systems",1981,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038215940&partnerID=40&md5=e7f6611aa423276ab95bd73aa45ad93e","Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB LightGBM. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy. © 2017 Neural information processing systems foundation. All rights reserved.",Conference Paper,"Final",Scopus,2-s2.0-85038215940
"Chen T., He T., Benesty M.","XGBoost: Extreme Gradient Boosting, R package version 0.4-3",2015,"Xgboost: Extreme Gradient Boosting",775,,[No abstract available],,"Final",Scopus,2-s2.0-84964778324
"Lu Y., Liu C., Wang K.I.-K., Huang H., Xu X.","Digital Twin-driven smart manufacturing: Connotation, reference model, applications and research issues",2020,"Robotics and Computer-Integrated Manufacturing",306,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070213247&doi=10.1016%2fj.rcim.2019.101837&partnerID=40&md5=9a0d5c501fce6b59922592814303e510","This paper reviews the recent development of Digital Twin technologies in manufacturing systems and processes, to analyze the connotation, application scenarios, and research issues of Digital Twin-driven smart manufacturing in the context of Industry 4.0. To understand Digital Twin and its future potential in manufacturing, we summarized the definition and state-of-the-art development outcomes of Digital Twin. Existing technologies for developing a Digital Twin for smart manufacturing are reviewed under a Digital Twin reference model to systematize the development methodology for Digital Twin. Representative applications are reviewed with a focus on the alignment with the proposed reference model. Outstanding research issues of developing Digital Twins for smart manufacturing are identified at the end of the paper. © 2019 Elsevier Ltd",Review,"Final",Scopus,2-s2.0-85070213247
"Gao S., Zhou M., Wang Y., Cheng J., Yachi H., Wang J.","Dendritic Neuron Model with Effective Learning Algorithms for Classification, Approximation, and Prediction",2019,"IEEE Transactions on Neural Networks and Learning Systems",303,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059111349&doi=10.1109%2fTNNLS.2018.2846646&partnerID=40&md5=ae47f4e3058c24b5e2d5680e48a1769b","An artificial neural network (ANN) that mimics the information processing mechanisms and procedures of neurons in human brains has achieved a great success in many fields, e.g., classification, prediction, and control. However, traditional ANNs suffer from many problems, such as the hard understanding problem, the slow and difficult training problems, and the difficulty to scale them up. These problems motivate us to develop a new dendritic neuron model (DNM) by considering the nonlinearity of synapses, not only for a better understanding of a biological neuronal system, but also for providing a more useful method for solving practical problems. To achieve its better performance for solving problems, six learning algorithms including biogeography-based optimization, particle swarm optimization, genetic algorithm, ant colony optimization, evolutionary strategy, and population-based incremental learning are for the first time used to train it. The best combination of its user-defined parameters has been systemically investigated by using the Taguchi's experimental design method. The experiments on 14 different problems involving classification, approximation, and prediction are conducted by using a multilayer perceptron and the proposed DNM. The results suggest that the proposed learning algorithms are effective and promising for training DNM and thus make DNM more powerful in solving classification, approximation, and prediction problems. © 2012 IEEE.",Article,"Final",Scopus,2-s2.0-85059111349
"Jones D., Snider C., Nassehi A., Yon J., Hicks B.","Characterising the Digital Twin: A systematic literature review",2020,"CIRP Journal of Manufacturing Science and Technology",184,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081219520&doi=10.1016%2fj.cirpj.2020.02.002&partnerID=40&md5=15c4de4919afc792ffa9fd0a75d72c9a","While there has been a recent growth of interest in the Digital Twin, a variety of definitions employed across industry and academia remain. There is a need to consolidate research such to maintain a common understanding of the topic and ensure future research efforts are to be based on solid foundations. Through a systematic literature review and a thematic analysis of 92 Digital Twin publications from the last ten years, this paper provides a characterisation of the Digital Twin, identification of gaps in knowledge, and required areas of future research. In characterising the Digital Twin, the state of the concept, key terminology, and associated processes are identified, discussed, and consolidated to produce 13 characteristics (Physical Entity/Twin; Virtual Entity/Twin; Physical Environment; Virtual Environment; State; Realisation; Metrology; Twinning; Twinning Rate; Physical-to-Virtual Connection/Twinning; Virtual-to-Physical Connection/Twinning; Physical Processes; and Virtual Processes) and a complete framework of the Digital Twin and its process of operation. Following this characterisation, seven knowledge gaps and topics for future research focus are identified: Perceived Benefits; Digital Twin across the Product Life-Cycle; Use-Cases; Technical Implementations; Levels of Fidelity; Data Ownership; and Integration between Virtual Entities; each of which are required to realise the Digital Twin. © 2020 University of Bristol",Article,"Final",Scopus,2-s2.0-85081219520
"Boje C., Guerriero A., Kubicki S., Rezgui Y.","Towards a semantic Construction Digital Twin: Directions for future research",2020,"Automation in Construction",113,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082386834&doi=10.1016%2fj.autcon.2020.103179&partnerID=40&md5=914efcd6c064d6a97299f00d812bb8fc","As the Architecture, Engineering and Construction sector is embracing the digital age, the processes involved in the design, construction and operation of built assets are more and more influenced by technologies dealing with value-added monitoring of data from sensor networks, management of this data in secure and resilient storage systems underpinned by semantic models, as well as the simulation and optimisation of engineering systems. Aside from enhancing the efficiency of the value chain, such information-intensive models and associated technologies play a decisive role in minimising the lifecycle impacts of our buildings. While Building Information Modelling provides procedures, technologies and data schemas enabling a standardised semantic representation of building components and systems, the concept of a Digital Twin conveys a more holistic socio-technical and process-oriented characterisation of the complex artefacts involved by leveraging the synchronicity of the cyber-physical bi-directional data flows. Moreover, BIM lacks semantic completeness in areas such as control systems, including sensor networks, social systems, and urban artefacts beyond the scope of buildings, thus requiring a holistic, scalable semantic approach that factors in dynamic data at different levels. The paper reviews the multi-faceted applications of BIM during the construction stage and highlights limits and requirements, paving the way to the concept of a Construction Digital Twin. A definition of such a concept is then given, described in terms of underpinning research themes, while elaborating on areas for future research. © 2020 The Authors",Review,"Final",Scopus,2-s2.0-85082386834
"Zhai B., Chen J.","Development of a stacked ensemble model for forecasting and analyzing daily average PM2.5 concentrations in Beijing, China",2018,"Science of the Total Environment",106,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045582819&doi=10.1016%2fj.scitotenv.2018.04.040&partnerID=40&md5=0ec66e8e268e3b2b057e79636ef431c7","A stacked ensemble model is developed for forecasting and analyzing the daily average concentrations of fine particulate matter (PM2.5) in Beijing, China. Special feature extraction procedures, including those of simplification, polynomial, transformation and combination, are conducted before modeling to identify potentially significant features based on an exploratory data analysis. Stability feature selection and tree-based feature selection methods are applied to select important variables and evaluate the degrees of feature importance. Single models including LASSO, Adaboost, XGBoost and multi-layer perceptron optimized by the genetic algorithm (GA-MLP) are established in the level 0 space and are then integrated by support vector regression (SVR) in the level 1 space via stacked generalization. A feature importance analysis reveals that nitrogen dioxide (NO2) and carbon monoxide (CO) concentrations measured from the city of Zhangjiakou are taken as the most important elements of pollution factors for forecasting PM2.5 concentrations. Local extreme wind speeds and maximal wind speeds are considered to extend the most effects of meteorological factors to the cross-regional transportation of contaminants. Pollutants found in the cities of Zhangjiakou and Chengde have a stronger impact on air quality in Beijing than other surrounding factors. Our model evaluation shows that the ensemble model generally performs better than a single nonlinear forecasting model when applied to new data with a coefficient of determination (R2) of 0.90 and a root mean squared error (RMSE) of 23.69 μg/m3. For single pollutant grade recognition, the proposed model performs better when applied to days characterized by good air quality than when applied to days registering high levels of pollution. The overall classification accuracy level is 73.93%, with most misclassifications made among adjacent categories. The results demonstrate the interpretability and generalizability of the stacked ensemble model. © 2018 The Authors",Article,"Final",Scopus,2-s2.0-85045582819
"Bianchini A., Bandini P.","Prediction of pavement performance through neuro-fuzzy reasoning",2010,"Computer-Aided Civil and Infrastructure Engineering",88,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-72549118772&doi=10.1111%2fj.1467-8667.2009.00615.x&partnerID=40&md5=c0c401cc14bc0929593f417c3e1a0d58","Government agencies and consulting companies in charge of pavement management face the challenge of maintaining pavements in serviceable conditions throughout their life from the functional and structural standpoints. For this, the assessment and prediction of the pavement conditions are crucial. This study proposes a neuro-fuzzy model to predict the performance of flexible pavements using the parameters routinely collected by agencies to characterize the condition of an existing pavement. These parameters are generally obtained by performing falling weight deflectometer tests and monitoring the development of distresses on the pavement surface. The proposed hybrid model for predicting pavement performance was characterized by multilayer, feedforward neural networks that led the reasoning process of the IF-THEN fuzzy rules. The results of the neuro-fuzzy model were superior to those of the linear regression model in terms of accuracy in the approximation. The proposed neuro-fuzzy model showed good generalization capability, and the evaluation of the model performance produced satisfactory results, demonstrating the efficiency and potential of these new mathematical modeling techniques. © 2009 Computer-Aided Civil and Infrastructure Engineering.",Article,"Final",Scopus,2-s2.0-72549118772
"Wang J., Kumbasar T.","Parameter optimization of interval Type-2 fuzzy neural networks based on PSO and BBBC methods",2019,"IEEE/CAA Journal of Automatica Sinica",83,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059837108&doi=10.1109%2fJAS.2019.1911348&partnerID=40&md5=ebd3aa37ee71fd2e8950b0e6f22ae206","Interval type-2 fuzzy neural networks IT2FNNs can be seen as the hybridization of interval type-2 fuzzy systems IT2FSs and neural networks NNs . Thus, they naturally inherit the merits of both IT2FSs and NNs. Although IT2FNNs have more advantages in processing uncertain, incomplete, or imprecise information compared to their type-1 counterparts, a large number of parameters need to be tuned in the IT2FNNs, which increases the difficulties of their design. In this paper, big bang-big crunch BBBC optimization and particle swarm optimization PSO are applied in the parameter optimization for Takagi-Sugeno-Kang TSK type IT2FNNs. The employment of the BBBC and PSO strategies can eliminate the need of backpropagation computation. The computing problem is converted to a simple feed-forward IT2FNNs learning. The adoption of the BBBC or the PSO will not only simplify the design of the IT2FNNs, but will also increase identification accuracy when compared with present methods. The proposed optimization based strategies are tested with three types of interval type-2 fuzzy membership functions IT2FMFs and deployed on three typical identification models. Simulation results certify the effectiveness of the proposed parameter optimization methods for the IT2FNNs. © 2014 Chinese Association of Automation.",Article,"Final",Scopus,2-s2.0-85059837108
"Gong H., Sun Y., Shu X., Huang B.","Use of random forests regression for predicting IRI of asphalt pavements",2018,"Construction and Building Materials",82,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053468185&doi=10.1016%2fj.conbuildmat.2018.09.017&partnerID=40&md5=98575b32406cb3356a0f133ce4b099b1","Random forest is a powerful machine learning algorithm with demonstrated success. In this study, the authors developed a random forests regression (RFR) model to estimate the international roughness index (IRI) of flexible pavements from distress measurements, traffic, climatic, maintenance and structural data. To validate the model, more than 11,000 samples were collected from the database of long-term pavement performance (LTPP) program, with 80% randomly sampled data for training and 20% of them for testing the RFR model. The performance of the RFR model was then compared with that of the regularized linear regression model. The results showed that the RFR model significantly outperformed the linear regression model, with coefficients of determination (R2) greater than 0.95 in both the training and test sets. The variable importance score obtained from the RFR revealed that the initial IRI was the most important factor affecting the development of the IRI. In addition, the transverse cracking, fatigue cracking, rutting, annual average precipitation and service age had important influences on the IRI. Other distresses such as longitudinal cracking, edge cracking, aggregate polishing, and potholes exerted little impact on the evolution of the IRI. © 2018 Elsevier Ltd",Article,"Final",Scopus,2-s2.0-85053468185
"Guyon I., Elisseeff A.","An introduction to feature extraction",2006,"Feature Extraction, Foundations and Applications",72,,[No abstract available],,"Final",Scopus,2-s2.0-70350225878
"Li Y., Yang Z., Chen X., Yuan H., Liu W.","A stacking model using URL and HTML features for phishing webpage detection",2019,"Future Generation Computer Systems",71,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057258839&doi=10.1016%2fj.future.2018.11.004&partnerID=40&md5=b42d10ef75e4c63e07e4c9ec322cf35e","In this paper, we present a stacking model to detect phishing webpages using URL and HTML features. In terms of features, we design lightweight URL and HTML features and introduce HTML string embedding without using the third-party services, making it possible to develop real-time detection applications. Furthermore, we devise a stacking model by combining GBDT, XGBoost and LightGBM in multiple layers, which enables different models to be complementary, thus improving the performance on phishing webpage detection. In particular, we collect two real-world datasets for evaluations, named as 50K-PD and 50K-IPD, respectively. 50K-PD contains 49,947 webpages with URLs and HTML codes. 50K-IPD contains 53,103 webpages with screenshots in addition to URLs and HTML codes. The proposed approach outperforms quite a few machine learning models on multiple metrics, achieving 97.30% on accuracy, 4.46% on missing alarm rate, and 1.61% on false alarm rate on 50K-PD dataset. On 50K-IPD dataset, the proposed approach achieves 98.60% on accuracy, 1.28% on missing alarm rate, and 1.54% on false alarm rate. © 2018 Elsevier B.V.",Article,"Final",Scopus,2-s2.0-85057258839
"Wang G., Qiao J., Bi J., Li W., Zhou M.","TL-GDBN: Growing Deep Belief Network with Transfer Learning",2019,"IEEE Transactions on Automation Science and Engineering",57,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054471522&doi=10.1109%2fTASE.2018.2865663&partnerID=40&md5=085b71cdc907c9f2e3e6fa926cfab15b","A deep belief network (DBN) is effective to create a powerful generative model by using training data. However, it is difficult to fast determine its optimal structure given specific applications. In this paper, a growing DBN with transfer learning (TL-GDBN) is proposed to automatically decide its structure size, which can accelerate its learning process and improve model accuracy. First, a basic DBN structure with single hidden layer is initialized and then pretrained, and the learned weight parameters are frozen. Second, TL-GDBN uses TL to transfer the knowledge from the learned weight parameters to newly added neurons and hidden layers, which can achieve a growing structure until the stopping criterion for pretraining is satisfied. Third, the weight parameters derived from pretraining of TL-GDBN are further fine-tuned by using layer-by-layer partial least square regression from top to bottom, which can avoid many problems of traditional backpropagation algorithm-based fine-tuning. Moreover, the convergence analysis of the TL-GDBN is presented. Finally, TL-GDBN is tested on two benchmark data sets and a practical wastewater treatment system. The simulation results show that it has better modeling performance, faster learning speed, and more robust structure than existing models. Note to Practitioners - Transfer learning (TL) aims to improve training effectiveness by transferring knowledge from a source domain to target domain. This paper presents a growing deep belief network (DBN) with TL to improve the training effectiveness and determine the optimal model size. Facing a complex process and real-world workflow, DBN tends to require long time for its successful training. The proposed growing DBN with TL (TL-GDBN) accelerates the learning process by instantaneously transferring the knowledge from a source domain to each new deeper or wider substructure. The experimental results show that the proposed TL-GDBN model has a great potential to deal with complex system, especially the systems with high nonlinearity. As a result, it can be readily applicable to some industrial nonlinear systems. © 2004-2012 IEEE.",Article,"Final",Scopus,2-s2.0-85054471522
"Heaton J., Parlikad A.K., Schooling J.","Design and development of BIM models to support operations and maintenance",2019,"Computers in Industry",45,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070275875&doi=10.1016%2fj.compind.2019.08.001&partnerID=40&md5=da555f42d13efa8179ea99e33cf75666","Building Information Modelling (BIM) is one of the most significant technological advancements in recent years that has been adopted by the design and construction industry. While BIM adoption is growing, it can be witnessed that adoption is relatively weak within operational and maintenance (O&M) organisations such as Estate and Infrastructure Management, who would ultimately gain the highest value from utilising BIM. While the challenges of BIM adoption are multifaceted, there is a recurring theme of poor data integration between BIM and existing information management systems. There is a clear gap of knowledge on how to structure a BIM model that allows its efficient use in the O&M phase. Furthermore, there is a lack of claritiy on how to exchange information from a BIM model into an Asset Information Model (AIM). This paper outlines a methodology that enables extraction of BIM-related data directly from a model into a relational database for integration with existing asset management systems. The paper describes the BIM model requirements, development of the extraction platform, database architecture and framework. Furthermore, a case study is presented to demonstrate the methodology. The case study demonstrates that if the BIM model is designed from the start with consideration for the O&M requirements, it can be exploited for development into an AIM. It also shows that a structured approach to object classification within a BIM model supports the efficient exchange of data directly from the BIM model. © 2019 The Authors",Article,"Final",Scopus,2-s2.0-85070275875
"Chen J., Yin J., Zang L., Zhang T., Zhao M.","Stacking machine learning model for estimating hourly PM2.5 in China based on Himawari 8 aerosol optical depth data",2019,"Science of the Total Environment",40,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071505455&doi=10.1016%2fj.scitotenv.2019.134021&partnerID=40&md5=cb2cbb3e07260455e905a5c427a1588f","Aerosol optical depth (AOD) from polar orbit satellites and meteorological factors have been widely used to estimate concentrations of surface particulate matter with an aerodynamic diameter &lt;2.5 μm (PM2.5). However, estimations with high temporal resolution remain lacking because of the limitations of satellite observations. Here, we used AOD data with a temporal resolution of 1 h provided by a geostationary satellite called Himawari 8 to overcome this problem. We developed a stacking model, which contained three submodels of machine learning, namely, AdaBoost, XGBoost and random forest, stacked through a multiple linear regression model. Then, we estimated the hourly concentrations of PM2.5 in Central and Eastern China. The accuracy evaluation showed that the proposed stacking model performed better than the single models when applied to the test set, with an average coefficient of determination (R2) of 0.85 and a root-mean-square error (RMSE) of 17.3 μg/m3. Model precision reached its peak at 14:00 (local time), with an R2 (RMSE) of 0.92 (12.9 μg/m3). In addition, the spatial and temporal distributions of PM2.5 in Central and Eastern China were plotted in this study. The North China Plain was determined to be the most polluted area in China, with an annual mean PM2.5 concentration of 58 μg/m3 during daytime. Moreover, the pollution level of PM2.5 was the highest in winter, with an average concentration of 73 μg/m3. © 2019 Elsevier B.V.",Article,"Final",Scopus,2-s2.0-85071505455
"Shim C.-S., Dang N.-S., Lon S., Jeon C.-H.","Development of a bridge maintenance system for prestressed concrete bridges using 3D digital twin model",2019,"Structure and Infrastructure Engineering",39,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066812168&doi=10.1080%2f15732479.2019.1620789&partnerID=40&md5=9ba1b50226ea35f615b5482a8a41562b","Preventive maintenance is increasingly becoming an essential strategy in the bridge industry owing to its proactive advantage of maintaining the structural sustainability during its entire service life. Several in-use bridges lack an appropriate regular maintenance solution, leading to extra cost during the operation stage. This paper proposes a new generation of the bridge maintenance system by using a digital twin model concept for more reliable decision-making. A detailed solution is proposed in this work to enhance the bridge maintenance process using a parallel solution: a maintenance information management system based on a 3D information model in conjunction with a digital inspection system using image processing. Three-dimensional digital models are required to utilise information from the entire lifecycle of a project, including design and construction, operation, and maintenance, by continuously exchanging and updating data from each stakeholder. For the maintenance of prestressed concrete bridges, the twin models are defined and their uses are presented. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.",Article,"Final",Scopus,2-s2.0-85066812168
"Bi J., Yuan H., Zhou M.","Temporal Prediction of Multiapplication Consolidated Workloads in Distributed Clouds",2019,"IEEE Transactions on Automation Science and Engineering",38,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077492061&doi=10.1109%2fTASE.2019.2895801&partnerID=40&md5=cd6f136645890da109db04b466974607","With their fast development and deployment, a large number of cloud services provided by distributed cloud data centers have become the most important part of Internet services. In spite of numerous benefits, their providers face some challenging issues, e.g., dynamic resource scaling and power consumption. Workload prediction plays a crucial role in addressing them. Accuracy and fast learning are the key performances. Its consistent efforts have been made for their improvement. This paper proposes an integrated prediction method that combines the Savitzky-Golay filter and wavelet decomposition with stochastic configuration networks to predict workload at the next time slot. In this approach, a task time series is first smoothed by the SG filter, and the smoothed one is then decomposed into multiple components via wavelet decomposition. Based on them, an integrated model is, for the first time, established and can well characterize the statistical features of both trend and detailed components. Experimental results demonstrate that it achieves better prediction results and faster learning speed than some representative prediction methods. Note to Practitioners-Workload prediction plays an important role in constructing scalable and green distributed cloud data centers. This paper presents a novel and fundamental methodology to achieve accuracy and fast learning for workload prediction. It develops an integrated prediction approach that combines the Savitzky-Golay filter and wavelet decomposition with stochastic configuration networks to predict workload at the next time slot. In order to establish a fine prediction model for the obtained information while achieving better prediction results and faster learning speed, this paper proposes an integrated method, SGW-S, to build a prediction model of a task time series and determine its optimal model parameters. The experimental results in the real-world data set show that the proposed method outperforms baseline methods in predicting the large-scale task time series. The proposed approach can aid the design and optimization of industrial cloud data centers and practitioners' prediction of different types of task time series. © 2019 IEEE.",Article,"Final",Scopus,2-s2.0-85077492061
"Pan N.-F., Ko C.-H., Yang M.-D., Hsu K.-C.","Pavement performance prediction through fuzzy regression",2011,"Expert Systems with Applications",38,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953695625&doi=10.1016%2fj.eswa.2011.02.007&partnerID=40&md5=91905630906193654e25da059f215d29","Accurate predictions of future pavement conditions are essential for determining the most cost-effective maintenance strategy. The current methods for assessing pavement conditions involve either equipment measures or visual inspections. Equipment measures are not extensively implemented because of high cost; thus, subjective evaluations by road inspectors are often used as a replacement. Nevertheless, visual inspections could draw in errors and variations due to subjectivity and uncertainty. The present serviceability index (PSI), one of the most common indicators used to evaluate pavement performance, is incapable of transforming one's imprecise judgment into an exact number between 0 (the worst) and 5 (the best). Conventional regression cannot deal with visual inspection data that are linguistic or non-crisp. In contrast, fuzzy regression is capable of handling such fuzzy data. In this paper, pavement conditions are exemplified by five membership functions and estimated by using fuzzy regression to better account the uncertainties of the traditional method. Also, a similarity indicator is applied to measure the goodness of fit. A case study using pavement inspection data is presented to establish estimated fuzzy regression equations. The results demonstrate the capability of the model, which is able to assist road administration units to determine desirable repair actions regarding the predicted pavement conditions. © 2011 Published by Elsevier Ltd.",Article,"Final",Scopus,2-s2.0-79953695625
"Ma Z., Cai S., Mao N., Yang Q., Feng J., Wang P.","Construction quality management based on a collaborative system using BIM and indoor positioning",2018,"Automation in Construction",37,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044618754&doi=10.1016%2fj.autcon.2018.03.027&partnerID=40&md5=78b0cfe233801365e3e76fd5b08e8dcc","Quality is one of the most vital aspects of a construction project, and inspection is the most important task in construction quality management. Despite the application of advanced information technologies, the omission of check items and in-efficiency of entering inspection results from paper-based inspection records into computers and collaboration among the construction stakeholders remain to be major problems. This paper proposes an approach to make the process of construction quality management more effective and collaborative by developing a system based on the integrated application of building information modeling (BIM) and indoor positioning technology. First, the system requirements of the collaboration platform are analyzed based on the standards for construction quality inspection in China, the technologies to be used for its implementation are justified, and a process model for the collaboration of multiple stakeholders is established. Next, the system architecture is developed, and the algorithm for generating inspection tasks and the technique for integrating with indoor positioning technology are formulated. Finally, the implementation of a prototype system is presented, and the effectiveness and efficiency of the approach for construction quality management are verified by using the system in an on-site test. © 2018 Elsevier B.V.",Article,"Final",Scopus,2-s2.0-85044618754
"Gong H., Sun Y., Mei Z., Huang B.","Improving accuracy of rutting prediction for mechanistic-empirical pavement design guide with deep neural networks",2018,"Construction and Building Materials",32,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054017039&doi=10.1016%2fj.conbuildmat.2018.09.087&partnerID=40&md5=124a12ed22000d73f56b2d40c868f34f","Rutting in asphalt pavement is a critical design criterion in the Mechanistic-Empirical Pavement Design Guide (MEPDG). However, many studies have shown that the rutting transfer function in the MEPDG fails to produce reliable predictions in that it calculates simply through a linear combination of the permanent deformations in all susceptible layers. To address this issue, the present study developed two deep neural network (NNs) that can be included in the MEPDG to improve the accuracy of rutting prediction: the first one (NN3) utilized the predicted rutting data by the MEPDG, respectively in the asphalt concrete (AC), granular base and subgrade as the primary inputs, while the other (NN20) further adopted seventeen additional parameters concerning the material, structure, traffic, and climate. To demonstrate the effectiveness of the presented NNs, two multiple linear regression (MLR) models, MLR3 and MLR20, using the same inputs for NN3 and NN20 but developed in the same way with the rutting transfer function in the MEPDG, were employed to act as a performance baseline. The results indicated that both the developed NNs, particularly the NN20, exhibited significantly better predictive performance than the two MLR models, regardless of whether they were in training or testing. As a complement to interpret the NN models, the importance measures from the random forest showed that the transfer function in the MEPDG may have excluded some crucial variables such as the air voids in the AC, and thus caused its unsatisfactory predictive performance. © 2018",Article,"Final",Scopus,2-s2.0-85054017039
"Ma Z., Liu Z.","BIM-based intelligent acquisition of construction information for cost estimation of building projects",2014,"Procedia Engineering",28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949127385&doi=10.1016%2fj.proeng.2014.10.561&partnerID=40&md5=e3421b149347b4b2594cedabce02dd75","Construction cost estimation, which is normally labor-intensive and error-prone, is one of the most important works in the architecture, engineering, construction, and facilities management industry during a project's lifecycle. With the development of information technology, it is expected that the efficiency and accuracy of cost estimation for tendering of building projects (TBP cost estimation for short hereafter) can be greatly improved if it can be carried out automatically based on the design results using building information modeling (BIM-based design results for short hereafter). One of the major obstacle to realize the automatic TBP cost estimation is the acquisition of the construction information (e.g. construction method, equipment), which are necessary for TBP cost estimation but often not included in the design results. This paper aims to establish an approach to identify and acquire the construction information systematically in order to facilitate the automatic TBP cost estimation based on BIM-based design results. First, the process of TBP cot estimation based on BQ method in China is introduced and the related specifications are classified. Then a typical specification for TBP cost estimation is analyzed to identify the construction information that are required in TBP cost estimation and a typical specification for the requirement on design detail fulfillment is analyzed to identify the construction information that can be obtained from the design result. Next, based on the above results, the construction information that need to be established during TBP cost estimation are highlighted and an approach to acquire them based on BIM-based design results are presented. Finally, the applicability of the approach is discussed. For simplicity, the scope for the analysis is confined to TBP cost estimation of concrete construction of reinforced concrete structures in this paper. © 2014 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license.",Conference Paper,"Final",Scopus,2-s2.0-84949127385
"Jiang M., Liu J., Zhang L., Liu C.","An improved Stacking framework for stock index prediction by leveraging tree-based ensemble models and deep learning algorithms",2020,"Physica A: Statistical Mechanics and its Applications",23,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077066213&doi=10.1016%2fj.physa.2019.122272&partnerID=40&md5=2e49ab42191d487c300fe3be12399390","Stock price index is an essential component of financial systems and indicates the economic performance in the national level. Even if a small improvement in its forecasting performance will be highly profitable and meaningful. This manuscript input technical features together with macroeconomic indicators into an improved Stacking framework for predicting the direction of the stock price index in respect of the price prevailing some time earlier, if necessary, a month. Random forest (RF), extremely randomized trees (ERT), extreme gradient boosting (XGBoost) and light gradient boosting machine (LightGBM), which pertain to the tree-based algorithms, and recurrent neural networks (RNN), bidirectional RNN, RNN with long short-term memory (LSTM) and gated recurrent unit (GRU) layer, which pertain to the deep learning algorithms, are stacked as base classifiers in the first layer. Cross-validation method is then implemented to iteratively generate the input for the second level classifier in order to prevent overfitting. In the second layer, logistic regression, as well as its regularized version, are employed as meta-classifiers to identify the unique learning pattern of the base classifiers. Empirical results over three major U.S. stock indices indicate that our improved Stacking method outperforms state-of-the-art ensemble learning algorithms and deep learning models, achieving a higher level of accuracy, F-score and AUC value. Besides, another contribution in our research paper is the design of a Lasso (least absolute shrinkage and selection operator) based meta-classifier that is capable of automatically weighting/selecting the optimal base learners for the forecasting task. Our findings provide an integrated Stacking framework in the financial area. © 2019 Elsevier B.V.",Article,"Final",Scopus,2-s2.0-85077066213
"Lu Q., Chen L., Li S., Pitt M.","Semi-automatic geometric digital twinning for existing buildings based on images and CAD drawings",2020,"Automation in Construction",22,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082486835&doi=10.1016%2fj.autcon.2020.103183&partnerID=40&md5=8940c891ecdf5aaa634ff403282d13d6","Despite the emerging new data capturing technologies and advanced modelling systems, the process of geometric digital twin modelling for existing buildings still lacks a systematic and completed framework to streamline. As-is Building Information Model (BIM) is one of the commonly used geometric digital twin modelling approaches. However, the process of as-is BIM construction is time-consuming and needed to improve. To address this challenge, in this paper, a semi-automatic approach is developed to establish a systematic, accurate and convenient digital twinning system based on images and CAD drawings. With this ultimate goal, this paper summarises the state-of-the-art geometric digital twinning methods and elaborates on the methodological framework of this semi-automatic geometric digital twinning approach. The framework consists of three modules. The Building Framework Construction and Geometry Information Extraction (Module 1) defines the locations of each structural component through recognising special symbols in a floor plan and then extracting data from CAD drawings using the Optical Character Recognition (OCR) technology. Meaningful text information is further filtered based on predefined rules. In order to integrate with completed building information, the Building Information Complementary (Module 2) is developed based on neuro-fuzzy system (NFS) and the image processing procedure to supplement additional building components. Finally, the Information Integration and IFC Creation (Module 3) integrates information from Module 1 and 2 and creates as-is Industry Foundation Classes (IFC) BIM based on IFC schema. A case study using part of an office building and the results of its analysis are provided and discussed from the perspectives of applicability and accuracy. Future works and limitations are also addressed. © 2020 Elsevier B.V.",Article,"Final",Scopus,2-s2.0-85082486835
"Feng L., Li Y., Wang Y., Du Q.","Estimating hourly and continuous ground-level PM2.5 concentrations using an ensemble learning algorithm: The ST-stacking model",2020,"Atmospheric Environment",20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077383032&doi=10.1016%2fj.atmosenv.2019.117242&partnerID=40&md5=aae6a2a71ec27c162618e0ff62c855b4","Estimation of hourly and continuous ground-level fine particulate matter (PM2.5) concentrations is essential for PM2.5 pollution sources identifications, targeted policy development and population exposure research. However, current PM2.5 estimation studies rely heavily on satellite-based aerosol optical depth (AOD) data, and the limited transit times of polar-orbiting satellites such as Terra and Aqua, nighttime gaps in data from geostationary satellites such as Himawari-8, and cloud contamination reported for both types of satellites challenge the estimation of spatiotemporally continuous PM2.5 concentrations. In this study, spatiotemporal PM2.5 characteristic was constructed by the spatiotemporal fusion method. Specifically, multi-source data, including spatiotemporal, periodic, meteorological, vegetation, anthropogenic and topological characteristics, were incorporated into an ensemble learning method that combined extreme gradient boosting (XGBoost), k-nearest neighbour (KNN) and back-propagation neural network (BPNN) algorithms in level 1 and used linear regression (LR) for integration in level 2. The optimized stacking strategy that considered PM2.5 spatiotemporal autocorrelation was called the ST-stacking model. The model was trained, validated and tested with data acquired for China in 2017. The ST-stacking model outperformed XGBoost, KNN and BPNN models by 9.27% on average, with an R2 = 0.9191. Using the model, the 24-h and continuous ground-level PM2.5 concentrations in mainland China on 11 May 2017 were mapped, and parts of Beijing and Chengdu were selected for more detailed analysis. The PM2.5 concentrations in Taklimakan Desert, North China Plain, Sichuan Basin and Yangtze Plain were much higher than those in other locations on this day, which was generally consistent with the long-term patterns reported in previous studies. © 2019 Elsevier Ltd",Article,"Final",Scopus,2-s2.0-85077383032
"Brown N.C., Jusiega V., Mueller C.T.","Implementing data-driven parametric building design with a flexible toolbox",2020,"Automation in Construction",18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086365450&doi=10.1016%2fj.autcon.2020.103252&partnerID=40&md5=055809488fb89446a05b75df1badb980","Designers in architecture and engineering are increasingly employing parametric models linked to performance simulations to assist in early building design decisions. This context presents a clear opportunity to integrate advanced functionality for engaging with quantitative design objectives directly into computational design environments. This paper presents a toolbox for data-driven design, which draws from data science and optimization methods to enable customized workflows for early design space exploration. It then applies these approaches to a multi-objective conceptual design problem involving structural and energy performance for a long span roof with complex geometry and considerable design freedom. The case study moves from initial brainstorming through design refinement while demonstrating the advantages of flexible workflows for managing design data. Through investigation of a realistic early design prompt, this paper reveals strengths, limitations, potential pitfalls, and future opportunities for data-driven parametric design. © 2020 Elsevier B.V.",Article,"Final",Scopus,2-s2.0-85086365450
"Collop A.C., Cebon D.","A model of whole-life flexible pavement performance",1995,"Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science",17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029535790&doi=10.1243%2fPIME_PROC_1995_209_170_02&partnerID=40&md5=004f2fdca08caefd1826b441c0ba5c6d","A new ‘whole-life’ pavement performance model (WLPPM), which is capable of making deterministic pavement damage predictions due to realistic traffic and environmental loading, has been developed. A vehicle simulation is used to generate dynamic tyre forces that are a function of distance along the road. These dynamic tyre forces are then combined with the appropriate pavement primary response influence functions (stress, strain and displacement) to give primary response histories at regularly spaced points along the pavement. The primary response histories are then transformed into pavement damage (fatigue and permanent deformation) using an appropriate damage model. The result is an increment of damage at each point along the pavement due to a single vehicle pass. The pavement surface profile is then updated to reflect permanent deformation damage and the layer material parameters are changed to reflect fatigue damage. The procedure is then repeated for the next vehicle pass. Particular attention is given to modelling strength variations in the pavement and dynamic tyre forces. The model is used to investigate the relationship between ‘hot spots’ (due to peak dynamic loads), ‘weak spots’ (due to initial pavement stiffness variations) and long-term pavement damage. © 1995, Institution of Mechanical Engineers. All rights reserved.",Review,"Final",Scopus,2-s2.0-0029535790
"Nie Z., Shen F., Xu D., Li Q.","An EMD-SVR model for short-term prediction of ship motion using mirror symmetry and SVR algorithms to eliminate EMD boundary effect",2020,"Ocean Engineering",15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089882999&doi=10.1016%2fj.oceaneng.2020.107927&partnerID=40&md5=27fa48c7dd8ce8269f5b8e8788aada20","Short-term prediction technology has a vital role in improving the efficacy and safety of several offshore operations. Motivated by nonlinear learning ability of support vector regression model (SVR model) and non-stationary data processing ability of empirical mode decomposition (EMD), this study offers a hybrid EMD-SVR model for short-term prediction of ship motion using mirror symmetry and SVR algorithm to eliminate EMD boundary effect. This model is abbreviated as the MSEMD-SVR model in this study. Even though EMD is efficient in dealing with non-stationary data, its boundary effect decreases the prediction accuracy. Raw data are initially processed by improved EMD and then predicted by SVR in the MSEMD-SVR model. This study confirms the negative EMD boundary effect on the prediction accuracy of classical EMD-SVR model and validity of the mirror symmetry method using the rolling and pitching of ship motion data collected during sailing for experiments. Based on the results of contrast experiments, the MSEMD-SVR model is more feasible and reliable for short-term prediction of ship motion than the EMD-SVR model, which does not deal with EMD boundary effect or only applies the mirror symmetry method to deal with. © 2020 Elsevier Ltd",Article,"Final",Scopus,2-s2.0-85089882999
"Yang X., Dindoruk B., Lu L.","A comparative analysis of bubble point pressure prediction using advanced machine learning algorithms and classical correlations",2020,"Journal of Petroleum Science and Engineering",15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075493164&doi=10.1016%2fj.petrol.2019.106598&partnerID=40&md5=b9ecc49dc588845de3a29e146d098cb4","The need for fluid properties or PVT (Pressure-Volume-Temperature) properties, is part of the entire Exploration and Production (E&amp;P) lifecycle from exploration to mature asset management to the typical later life events such as, Improved Oil Recovery (IOR). As the projects mature, the need for such data and its integration for various discipline-specific workflows and its interpretation in the light of reservoir performance varies. Among all the key PVT properties, bubble point pressure is probably the most important parameter. Bubble point pressure is important because it is the point at which constant composition and variable composition portions of the depletion paths merge. Geometrically, bubble point pressure appears to be a discontinuity. In addition, it dictates the existence (or not) of the incipient phase (i.e., gas phase) leading to the changes in the flow characteristics both in porous media and as well as within the wellbore and the facilities. Furthermore, it is also a good indicative of a possible gas cap when the reservoir is at saturation (reservoir pressure is equal to the bubble point pressure) or near-saturated. Among the highlighted uses, there are many more used such as the determination of the elements of miscibility, gas lift design, etc. Therefore, it is very important to estimate the bubble point pressure accurately. In this study, tree-based advanced machine learning algorithm including XGBoost, LightGBM, and random forest regressor, and multi-layer perceptron (neural network) regressor are implemented to predict bubble point pressure (Pbp). A novel super learner model which is also known as stacking ensemble is used to enhance base machine learning model performance on predicting bubble point pressure. Three datasets with different predictors are prepared to study machine learning algorithms' performance for three situations: only compositional data are available; only bulk properties (Gas-Oil-Ratio, gas gravity, API gravity and reservoir Temperature) are available; both compositional data and bulk properties are available. Through literature review, there is no research on using only compositional data and temperature to predict bubble point pressure. Our super learner model offers an accurate solution for oil bubble point pressure when only compositional data and temperature are available. Machine learning models perform better than empirical correlations with limited input data (i.e., bulk properties). When compositional data and bulk properties are all used as predictors, super learner reaches about 5.146% mean absolute relative error on predicting the bubble point pressure from global samples with bubble point pressures in the range of 100 to 10,000 psi, which is a wider range compared to most ANN models published in literature. © 2019 Elsevier B.V.",Article,"Final",Scopus,2-s2.0-85075493164
"Wang Z., Rezazadeh Azar E.","BIM-based draft schedule generation in reinforced concrete-framed buildings",2019,"Construction Innovation",15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063995374&doi=10.1108%2fCI-11-2018-0094&partnerID=40&md5=5613ef6170e0ea631f8cfb7c7366339a","Purpose: Project schedules have a vital role in the effective management of time, cost, scope and resources in construction projects, and creating schedules requires schedulers with construction knowledge and experience. The increase in the complexity of building projects and the emergence of building information modeling (BIM) in the architecture, engineering and construction industry have encouraged researchers to explore BIM capabilities for automated schedule generation. The scope and capabilities of the developed systems, however, are limited and the link between design and scheduling is still underdeveloped. This paper aims to investigate methods to develop a BIM-based framework to automatically generate schedules for concrete-framed buildings. Design/methodology/approach: This system first extracts the required data from the building information model, including elements’ dimensions, quantities, spatial information, materials and other related attributes. It then applies construction rules, prior knowledge and production rate data to create project work-packages, calculate their durations and determine their relationships. Finally, it organizes these results into a schedule using project management software. Findings: This system provides an automated and easy-to-use approach to generate schedules for concrete-framed buildings that are modeled in a BIM platform. It provides two schedules for each project, both a sequential and an overlapped solution, which the schedulers can modify into a practical schedule based on conditions and available resources. Originality/value: This research project presents an innovative approach to use BIM-based attributes of structural elements to develop list of work-packages and estimate their durations, and then it uses a combination of rule-based and case-based reasoning to generate the schedules. © 2019, Emerald Publishing Limited.",Article,"Final",Scopus,2-s2.0-85063995374
"Liu S., Bao J., Lu Y., Li J., Lu S., Sun X.","Digital twin modeling method based on biomimicry for machining aerospace components",2020,"J Manuf Syst",14,,[No abstract available],,"Final",Scopus,2-s2.0-85101788577
"Zhang D., Birgisson B., Luo X., Onifade I.","A new long-term aging model for asphalt pavements using morphology-kinetics based approach",2019,"Construction and Building Materials",14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072683646&doi=10.1016%2fj.conbuildmat.2019.117032&partnerID=40&md5=45a58c40e530d7c110674b3bb560e7f1","Asphalt pavements inevitably experience the long-term oxidative aging due to exposure under field conditions, which severely degrades the pavement performance and serviceability. Thus, accurate prediction of the long-term aging is important. Most studies on long-term aging prediction exhibit both systematic bias and data scatter due to their weakness of fully characterizing the field aging condition. This study presents a new long-term aging model for asphalt pavements, which focuses on the pavement binder viscosity as the target property and makes use of combined kinetics and mixture morphology framework for the model development. In this new model, the two-stage aging kinetics, representing the fast and constant aging reaction rates, are utilized to capture the oxidation mechanism of the long-term aging, which have the default values for different climate zones; the rheological kinetics are introduced to evaluate the temperature sensitivity of the long-term aging; a morphology parameter (i.e. primary structure coating thickness) that quantifies the asphalt mastic coating thickness on the load-bearing structure in the mixture is incorporated to characterize the effects of mixture morphology on the long-term aging. Besides, an energy-based pavement temperature model that considers climatic properties and site-specific pavement parameters is employed to determine the pavement field aging temperature. A large data set extracted from the Long-Term Pavement Performance (LTPP) database is used for the global model coefficients determination as well as the model validation, which takes up of 85% and 15% of the data set, respectively. A statistical analysis is performed to evaluate the model prediction accuracy, which indicates that the new model is able to accurately predict the long-term aged viscosity without any significant bias. In this regard, it is believed that the new long-term aging predictive model is a suitable candidate for the long-term aging prediction of asphalt pavements. © 2019 Elsevier Ltd",Article,"Final",Scopus,2-s2.0-85072683646
"Chao J., Jinxi Z.","Prediction Model for Asphalt Pavement Temperature in High-Temperature Season in Beijing",2018,"Advances in Civil Engineering",14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051325843&doi=10.1155%2f2018%2f1837952&partnerID=40&md5=b21030e6c2abf0291e10f292651836b1","Asphalt pavement temperatures greatly influence on the bearing capacity and performance, especially in high-temperature season. The variation rules of pavement temperatures under the high-temperature range affect the design and maintenance management of the asphalt pavement, as well as the accurate prediction for pavement temperatures. However, asphalt pavement temperature is greatly affected by various strongly correlated environmental factors and cannot be measured directly or predicted effectively. In this project, temperature sensors were embedded in the pavement of in-service road to collect temperature data by continuous record measurement, and regression model was conducted by the partial least squares method through comprehensive analysis on the pavement temperature data and synchronously environmental data from local weather station measured in July 2013, July 2014, and July 2015. The quantitative relationships in high-temperature season between environmental factors and pavement temperature were determined, and a model was established to predict the temperature of asphalt pavement based on environmental data. The model was verified by the recorded data from July 1, 2016, to July 31, 2016, and the results indicated that the pavement temperature can be predicted accurately and reliably by the proposed model. © 2018 Jing Chao and Zhang Jinxi.",Article,"Final",Scopus,2-s2.0-85051325843
"Sami Ur Rehman M., Thaheem M.J., Nasir A.R., Khan K.I.A.","Project schedule risk management through building information modelling",2020,"International Journal of Construction Management",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079812405&doi=10.1080%2f15623599.2020.1728606&partnerID=40&md5=09cacc443523449a4fb0f6c4f156b695","The complex and dynamic nature of construction projects, due to intricate design and involvement of several direct and indirect stakeholders, exposes them to various risks. These risks may cause cost and time overruns. A construction schedule is instrumental in monitoring the whole process. Past research has identified various factors causing project delays and several techniques have been developed to mitigate their effects. Recently, Building Information Modelling (BIM) is remarked as an efficient and reliable construction management tool. This study investigates the role of BIM in identifying schedule risks and providing an effective solution for schedule management. Thus, key project delay factors and their remedying BIM features have been studied through literature and primary data, and a factor-feature matrix is developed. To gauge the actual effectiveness of the studied aspects, a case study has been developed. It is found that BIM has a substantial impact on schedule management of construction projects. © 2020, © 2020 Informa UK Limited, trading as Taylor & Francis Group.",Article,"Article in Press",Scopus,2-s2.0-85079812405
"Liu B., Yan S., You H., Dong Y., Li Y., Lang J., Gu R.","Road surface temperature prediction based on gradient extreme learning machine boosting",2018,"Computers in Industry",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045064697&doi=10.1016%2fj.compind.2018.03.026&partnerID=40&md5=ef022d1393614ae3b431f0a29835bfff","The expressway is extremely important to transportation, but high road-surface temperatures (RST) can cause many traffic accidents. Most of the hourly RST prediction models are based on numerical methods, but the parameters are difficult to determine. Statistical methods cannot achieve the desired accuracy. To address these problems, this paper proposes a machine learning algorithm that utilizes gradient-boosting to assemble a ReLU (rectified linear unit)/softplus Extreme Learning Machine (ELM). By using historical data from the airport and Badaling expressways collected between November 2012 and September 2014, sigmoid ELM, ReLU ELM, softplus ELM, ReLU gradient ELM boosting (GBELM) and softplus GBELM were applied for RST forecasting, RMSE (root mean squared error), PCC (Pearson Correlation Coefficient), and the accuracy of these methods were analyzed. The experimental results show that ReLU/softplus can improve the performance of traditional ELM, and gradient boosting can further improve its performance. Thus, we obtain a more accurate model that utilizes GBELM with ReLU/softplus to forecast RST. For the airport expressway, our proposed model achieves an RMSE within 3 °C, an accuracy of 81.8% and a PCC of 0.954. For the Badaling expressway, our model achieves an RMSE within 2 °C, an accuracy of 87.4% and a PCC of 0.949. © 2018 Elsevier B.V.",Article,"Final",Scopus,2-s2.0-85045064697
"Abed A., Thom N., Neves L.","Probabilistic prediction of asphalt pavement performance",2019,"Road Materials and Pavement Design",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063130148&doi=10.1080%2f14680629.2019.1593229&partnerID=40&md5=117924617d00004c7204e27e1571cc14","Variability of pavement design parameters has always been a concern to pavement designers and highway agencies. A robust pavement design should take into account the variability of the design inputs and its impact on the reliability of the design. In this study, the variability effect of thickness and stiffness of pavement layers was investigated. The variability of these parameters was described by their mean values, standard deviations and probability distribution functions. Monte Carlo Simulation method was utilised to incorporate variability of the design parameters and to construct the probability distribution function of the outputs. KENLAYER software was used to calculate pavement response at predetermined critical locations; pavement reponse was then used to predict pavement performance regarding permanent deformation, bottom-up and top-down fatigue cracking by using the mechanistic empirical pavement design guide (MEPDG) models. A Matlab code was developed to run that analysis and obtain the probability distribution function of pavement performance indicators over time. It was found that the variability of pavement layer thickness and stiffness has a significant impact on pavement performance. Also, it was found that not only the mean of the predicted performance indicators is increasing over time, but the variance of these indicators is also increasing. This means that pavement condition cannot be described by the mean values of the indicators but by the probability distribution function which can describe pavement condition at any reliability level. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.",Article,"Final",Scopus,2-s2.0-85063130148
"Shirzad S., Aguirre M.A., Bonilla L., Elseifi M.A., Cooper S., Mohammad L.N.","Mechanistic-empirical pavement performance of asphalt mixtures with recycled asphalt shingles",2018,"Construction and Building Materials",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034973129&doi=10.1016%2fj.conbuildmat.2017.11.114&partnerID=40&md5=31e67fac7654d20cea6b45d6c7d88858","As highway agencies are in the process of adopting the new mechanistic-empirical pavement design guide, Pavement ME, it is unclear how asphalt mixtures incorporating Recycled Asphalt Shingle (RAS) will influence the design when mechanistic-empirical approaches are used. In this study, Pavement ME was used to evaluate the effects of RAS (with or without recycling agents [RAs]) on pavement performance. Furthermore, a cost analysis was conducted to assess the life-cycle cost of asphalt pavements constructed with RAS. Three different pavement structures were analyzed at three traffic levels (low, medium, and high) and for two climatic regions (cold and hot). Pavement ME predicted that the mix with 5% Post-Consumer Waste Shingle (PCWS) and 5% RA would be the best performer against roughness, rutting, and fatigue cracking. This is due to the stiffening effect of RAS, which is reflected in the dynamic modulus inputted in the software. While one would expect RAS to improve rutting performance, the superior fatigue performance of mixes with RAS was not expected given that the binder in RAS is an air-blown asphalt binder with poor elongation and relaxation characteristics. Results were compared to the Semi-Circular Bending (SCB) test results, which realistically predicted that the mixes with RAS and recycling agents would be the worst performers against cracking. This can be explained by the increased availability of aged RAS binder in these mixes when a recycling agent is used. Results of the cost analysis showed that the mixtures with RAS are more economical to produce. When considering the predicted performance of the mixes, the mix with 5% PCWS and 5% RA had the lowest cost over the pavement service life. © 2017",Article,"Final",Scopus,2-s2.0-85034973129
"Adi G.Y.N.N., Tandio M.H., Ong V., Suhartono D.","Optimization for Automatic Personality Recognition on Twitter in Bahasa Indonesia",2018,"Procedia Computer Science",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053154576&doi=10.1016%2fj.procs.2018.08.199&partnerID=40&md5=d172a0ee55eeb7cd91488f74b77ccb84","This paper presents optimization techniques for automatic personality recognition (APR) based on Twitter in Bahasa Indonesia, the mother tongue of Indonesians. Foremost, we discuss Twitter and its utilization as a resource for many types of research. Several previous studies have been attempted to predict users' personality automatically. However, only a few of them have done their research for Bahasa Indonesia data. Therefore, this paper discusses the optimization of APR in Bahasa Indonesia. We evaluate a series of techniques implementing hyperparameter tuning, feature selection, and sampling to improve the machine learning algorithms used. The personality prediction system is built on machine learning algorithms. There are three machine learning algorithms used in this study, namely Stochastic Gradient Descent (SGD), and two ensemble learning algorithms, Gradient Boosting (XGBoost), and stacking (super learner). By implementing this series of optimization techniques, the current study's evaluation results show huge improvement by achieving 1.0 ROC AUC score with SGD and Super Learner. © 2018 The Authors. Published by Elsevier Ltd.",Conference Paper,"Final",Scopus,2-s2.0-85053154576
"Ma X., Dong Z., Chen F., Xiang H., Cao C., Sun J.","Airport asphalt pavement health monitoring system for mechanical model updating and distress evaluation under realistic random aircraft loads",2019,"Construction and Building Materials",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069901264&doi=10.1016%2fj.conbuildmat.2019.07.174&partnerID=40&md5=fd3887ab2f78896bb85bd7833f619d89","The performance of airport asphalt pavement gradually deteriorates under aircraft load repetitions. In this study, a framework of airport asphalt pavement health monitoring system was developed to evaluate pavement performance under realistic random aircraft loads. The taxiway of the Beijing Capital International Airport was used as an example, and sensor layout was designed to obtain pavement mechanical responses and identify random aircraft loads. Multi-layered system theory was selected as the mechanical model to simulate airport asphalt pavement structure considering its computational efficiency. Mechanical model updating and distress evaluation based on airport asphalt pavement health monitoring system were discussed. First, in accordance with the theoretical relationship between mechanical responses and moduli derived from the mechanical model, the moduli of the pavement structure layers were back-calculated using measured mechanical responses. The Kalman filtering method was developed to identify modulus parameters that can be used to update the mechanical model. Second, full-field pavement mechanical responses under each load were reconstructed from the mechanical responses of a single point acquired by the sensors based on the updated mechanical model with knowledge of random aircraft loads and modulus parameters. Finally, real-time deterministic pavement distress calculation (cumulative damage from fatigue cracking and rutting) at any point were realized based on transfer functions between mechanical responses and performance. The strategy and process for short-term non-deterministic distress prediction were developed, and non-deterministic pavement distress at any point after multiple random aircraft loads can then be obtained. This study is beneficial for the understanding of airport pavement behavior and decision-making regarding pavement maintenance. © 2019 Elsevier Ltd",Article,"Final",Scopus,2-s2.0-85069901264
"Hu Q., Cai M., Mohabbati-Kalejahi N., Mehdizadeh A., Yazdi M.A.A., Vinel A., Rigdon S.E., Davis K.C., Megahed F.M.","A review of data analytic applications in road traffic safety. Part 2: Prescriptive modeling",2020,"Sensors (Switzerland)",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079571272&doi=10.3390%2fs20041096&partnerID=40&md5=87375dd8b110163985e44752e6638af0","In the first part of the review, we observed that there exists a significant gap between the predictive and prescriptive models pertaining to crash risk prediction and minimization, respectively. In this part, we review and categorize the optimization/ prescriptive analytic models that focus on minimizing crash risk. Although the majority of works in this segment of the literature are related to the hazardous materials (hazmat) trucking problems, we show that (with some exceptions) many can also be utilized in non-hazmat scenarios. In an effort to highlight the effect of crash risk prediction model on the accumulated risk obtained from the prescriptive model, we present a simulated example where we utilize four risk indicators (obtained from logistic regression, Poisson regression, XGBoost, and neural network) in the k-shortest path algorithm. From our example, we demonstrate two major designed takeaways: (a) the shortest path may not always result in the lowest crash risk, and (b) a similarity in overall predictive performance may not always translate to similar outcomes from the prescriptive models. Based on the review and example, we highlight several avenues for future research. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.",Review,"Final",Scopus,2-s2.0-85079571272
"Lintonen T., Raty T.","Self-learning of multivariate time series using perceptually important points",2019,"IEEE/CAA Journal of Automatica Sinica",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074983963&doi=10.1109%2fJAS.2019.1911777&partnerID=40&md5=d50d01d7696f1901fc15f425f79e8fd4","In machine learning, positive-unlabelled PU learning is a special case within semi-supervised learning. In positive-unlabelled learning, the training set contains some positive examples and a set of unlabelled examples from both the positive and negative classes. Positive-unlabelled learning has gained attention in many domains, especially in time-series data, in which the obtainment of labelled data is challenging. Examples which originate from the negative class are especially difficult to acquire. Self-learning is a semi-supervised method capable of PU learning in time-series data. In the self-learning approach, observations are individually added from the unlabelled data into the positive class until a stopping criterion is reached. The model is retrained after each addition with the existent labels. The main problem in self-learning is to know when to stop the learning. There are multiple, different stopping criteria in the literature, but they tend to be inaccurate or challenging to apply. This publication proposes a novel stopping criterion, which is called Peak evaluation using perceptually important points, to address this problem for time-series data. Peak evaluation using perceptually important points is exceptional, as it does not have tunable hyperparameters, which makes it easily applicable to an unsupervised setting. Simultaneously, it is flexible as it does not make any assumptions on the balance of the dataset between the positive and the negative class. © 2014 Chinese Association of Automation.",Article,"Final",Scopus,2-s2.0-85074983963
"Lin F., Jiang J., Fan J., Wang S.","A stacking model for variation prediction of public bicycle traffic flow",2018,"Intelligent Data Analysis",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049648962&doi=10.3233%2fIDA-173443&partnerID=40&md5=3137f7edd5740178b7919b3bba0c02fc","Public bicycle system can improve the public transport travel efficiency and reduce environmental pollution, which has been deployed in many cities all over the world. However, the bicycle usages become quite skewed and imbalanced in different stations. A system which could recommend the nearest available stations for passengers whether they are looking for a dock or a bike, is of great importance. Monitoring the current number of docks or bikes at each station cannot tackle this problem because it's too late to recommend the station for passengers to rent or return bikes after the imbalance has occurred. To address this issue, we propose a stacking model for variation prediction of public bicycle traffic flow called SMVP based on the real-world datasets. The stacking model integrates multiple base models which we trained by different combinations of features so that it could get better performance. We adopt a machine learning system called XGBoost [25] to train the models and construct the multiple complex factors which impact the public bicycle traffic flow. The traditional factors, such as temporal, spatial, historical and meteorological factors are taken into consideration. A new clustering factor which considers both the geographical positions and transition patterns of stations is also proposed in this framework and then we use the K-Medoids algorithm [12] to cluster stations into groups by constructing a new different station relation matrix which considers these two factors as the distance between different stations. The performance of SMVP is improved on the datasets of Hangzhou and New York City, especially in terms of Coefficient of Determination improved by 25.58% in Hangzhou, compared with the traditional stacking [5] and single model respectively. © 2018-IOS Press and the authors. All rights reserved.",Article,"Final",Scopus,2-s2.0-85049648962
"Chen X., Dong Q., Gu X., Mao Q.","Bayesian analysis of pavement maintenance failure probability with Markov chain Monte Carlo simulation",2019,"Journal of Transportation Engineering Part B: Pavements",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060755022&doi=10.1061%2fJPEODX.0000107&partnerID=40&md5=7f14a58b82bf4df235e5a5a2edeb6a5c","This study presented a Bayesian logistic model to evaluate the failure probability of asphalt pavement preventive treatments. The Markov Chain Monte Carlo (MCMC) simulation using Metropolis-Hasting sampling was adopted for the Bayesian analysis. Pavement performance data and other related information, including traffic level, climate and pavement structure, were collected from the long-term pavement performance experiments for the analysis. Four preventive maintenance treatment methods, including asphalt overlay, chip seal, fog seal and slurry seal, were compared. Both a logistic model and a Bayesian logistic model with MCMC simulation were developed. Compared with the logistic model, the Bayesian logistic model can greatly reduce the uncertainty of parameter estimates. In addition, by setting the previous distribution of the parameters, the estimates can be in accordance with practical experience or previous research after Bayesian analysis. Therefore, some abnormal estimates can be corrected. Both models suggest that the pretreatment pavement condition is the most significant factor for the failure of maintenance treatments. Generally, severe climate, traffic, or poor structural capacity increased the failure probability of pavement treatments. As for the four treatments, fog seal and slurry seal performed significantly poorer than asphalt overlay and chip seal. © 2019 American Society of Civil Engineers.",Article,"Final",Scopus,2-s2.0-85060755022
"Gong H., Sun Y., Hu W., Huang B.","Neural networks for fatigue cracking prediction using outputs from pavement mechanistic-empirical design",2019,"Int. J. Pavement Eng.",5,,[No abstract available],,"Final",Scopus,2-s2.0-85100074271
"Zhou S., Sun L., Xing W., Feng G., Ji Y., Yang J., Liu S.","Hyperspectral imaging of beet seed germination prediction",2020,"Infrared Physics and Technology",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085518315&doi=10.1016%2fj.infrared.2020.103363&partnerID=40&md5=236165bd9eebc7a915345f88ed8a1bcc","The germination prediction of beet seeds is of great significance for sugar beet cultivation. In this study, the near-infrared hyperspectral images of 3072 beet seeds and their corresponding germination states were obtained. We preprocessed the images by cutting, threshold segmentation, dilate, hole-removal, contour extraction, and extracted the average spectrum as the characteristic spectrum. After multi-parameter evaluation and analysis of the six methods, 2D pretreatment was used for mean spectrum. According to the node information gain of the constructed tree model and physical property analysis of beet seeds, 15 characteristic wavelengths were extracted to reduce the dimensionality of data analysis and SVM (RBF), random forest, LightGBM classification models were established respectively. After comprehensive analysis of the prediction effect, LightGBM model was re-established to predict the germination of beet seeds. Analyze external data through testing, founding that the predicted model has a classification prediction accuracy of 89% for the test set. The results show that the germination of beet seeds can be predicted accurately by using the technique of hyperspectral imaging and propose a new idea for batch online non-destructive testing of sugar beet seeds. © 2020 Elsevier B.V.",Article,"Final",Scopus,2-s2.0-85085518315
"Li R., Huang Y., Wang J.","Long-Term Traffic Volume Prediction Based on Type-2 Fuzzy Sets with Confidence Interval Method",2019,"International Journal of Fuzzy Systems",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074203444&doi=10.1007%2fs40815-019-00701-2&partnerID=40&md5=2cc0caff473cd0f14e1225ab7ca48d07","The paper uses Gaussian Interval Type-2 fuzzy sets theory to deal with the 24-h historical traffic volume data with uncertainty and randomness to get the 24-h prediction of traffic volume with higher precision. The central limit theorem is adopted to convert point data of mass traffic flow in sampling time period into interval data (also called confidence interval) which is being used to get the state variables of traffic volume. The historical traffic volume data are divided into a couple of intervals according to the similarity degree of the same sampling period for each day. Each interval represents a traffic state. Take these traffic states as data foundation, Markov chains model is built. Viterbi algorithm is used to find the maximum possible state sequence by decoding from Markov chain model by which the fuzzy inference system is constructed. The confidence interval data retain the uncertainty and randomness of traffic flow, meanwhile reduce the influence of noise from the detection data through the transition from traffic volume data to traffic state data. Fuzzy inference system based on Markov chains and Viterbi algorithm discusses the inherent relevance of traffic volume data. The combined Markov Gaussian Interval Type-2 fuzzy sets leads to interval forecasting traffic volume output with the ability of describing the possible range of the traffic volume as well as the traffic volume prediction data with high accuracy. The simulation results based on actual data show the validity and rationality of the model. © 2019, Taiwan Fuzzy Systems Association.",Article,"Final",Scopus,2-s2.0-85074203444
"Eghbalpoor R., Baghani M., Shahsavari H.","An implicit finite element framework considering damage and healing effects with application to cyclic moving load on asphalt pavement",2019,"Applied Mathematical Modelling",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060548065&doi=10.1016%2fj.apm.2019.01.021&partnerID=40&md5=b30cc46907822457c1d8a0646d95185d","In this work, the effect of healing process on the damage recovery and delaying the permanent deformation in smart asphalt pavements is numerically studied. In order to analyze the effect of healing, a 3D finite element model of asphalt pavement is presented under high cycle vehicles movement loadings. To study the mechanical behavior of the pavements, a thermodynamically consistent constitutive model has been discretized implicitly and implemented into the commercial FE software ABAQUS. In this regard, the implicit time integration scheme of the constitutive equations and corresponding material consistent tangent matrix are presented. Also, the Newton–Raphson iteration procedure is employed to calculate the internal variables such as permanent deformation, damage and healing parameters at each time increment. The results show that the healing affects the lifetime and load capacity of the pavements and leads to such a reduction in damage growth rate. Furthermore, damage recovery capability is degraded with the growth of permanent deformation and damage. It is demonstrated that without considering the healing effects, the model prediction underestimates the pavement lifetime with higher damage growth rate which is not a valid observation. © 2019",Article,"Final",Scopus,2-s2.0-85060548065
"Systèmes D.","Meet Virtual Singapore, the city's 3D digital twin",2019,"Meet Virtual Singapore, the City's 3D Digital Twin",4,,[No abstract available],,"Final",Scopus,2-s2.0-85097752653
"Meng H., Wang X., Wang X.","Expressway crash prediction based on traffic big data",2018,"ACM International Conference Proceeding Series",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062771313&doi=10.1145%2f3297067.3297093&partnerID=40&md5=7475c85c1a3c527ded2bfb32fd296b65","With the development of society, the number of vehicles increases rapidly. The vehicle plays an important role in people's life, however the problem of traffic safety caused by vehicles has also become increasingly prominent. In China, the high crash rate and casualty rate on expressways have always troubled traffic management department. So crash prediction on expressway becomes vital. Conventionally, crash prediction is based on traffic flow data. These data do not contain all the necessary factors. In this paper, we propose a method of prediction using real-world data, including historical accident data, road geometry data, vehicle speed data, and weather data. We treat the crash prediction problem as a binary classification problem. For classification, sample imbalanced is a great challenge in practice. Modifying sample weights is applied to handle this challenge. Three machine learning classification techniques, namely Random Forest (RF), Gradient Boosting Decision Tree (GBDT) and Xgboost, are considered to carry out the crash prediction task respectively. The best recall and precision rate of these models are respectively 0.764253 and 0.01062. The proposed method can be integrated into urban traffic control systems toward police dispatch and crash prevention. © 2018 ACM.",Conference Paper,"Final",Scopus,2-s2.0-85062771313
"Kitaha A., Biligiri K.P.","Prioritization-optimization process algorithm to manage pavement networks during the non-availability of historical data",2017,"Journal of Testing and Evaluation",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029091909&doi=10.1520%2fJTE20150287&partnerID=40&md5=479dcc9e6fa480fffd20ba7580cf7726","A common practice followed to rate the pavement surface condition is to use the ASTM D5340-12 procedure and estimate the pavement condition index (PCI) that helps prioritize maintenance needs and assists developing a pavement management system (PMS). However, ASTM-PCI that is dependent on time-based evaluation may not be suitable where there is a lack of historical records and it is being undertaken for the first time. To estimate a pavement's current PCI during the non-availability of historical data and establish a PMS incorporated with prioritized maintenance and optimized budget, a rational engineering criteria (EC) based methodological approach is required that is as robust as ASTM D5340-12. Thus, the objective of this study was to develop a rational EC-based prioritization optimization process PMS (POPMS) algorithm for a network that prioritizes maintenance strategies for identified pavement distresses and hence optimizes maintenance costs depending on budgetary allocations. An EC-based POPMS algorithm was based on a network length of 100.55 km, which followed prioritization-optimization process for single to multi-year programs. EC-PCI of pavement sections were estimated on the basis of segmented maintenance strategies including preventive and routine maintenance and reconstruction. EC-PCI was found to be rational since three distinct threshold zones were considered that could directly assign pavement maintenance strategies, which was straightforward and circularly referenced. Overall, POPMS algorithm facilitates practitioners to modify the threshold EC-based parameters that will estimate an optimal single/multiyear budget for maintenance as per the newly set threshold level, thus creating a whole new promising approach in the areas of roadway network level maintenance programs. Copyright © 2016 by ASTM International.",Article,"Final",Scopus,2-s2.0-85029091909
"Kissell R.","Algorithmic trading methods: applications using advanced statistics, optimization, and machine learning techniques",2020,"Lgorithmic Trading Methods: Applications Using Advanced Statistics, Optimization, and Machine Learning Techniques",3,,[No abstract available],,"Final",Scopus,2-s2.0-85097771070
"Pavlov Y.L.","Random forests",2019,"Machine Learning",3,,[No abstract available],,"Final",Scopus,2-s2.0-85097777468
"Mechelli A., Vieira S.","Machine learning: Methods and applications to brain disorders",2019,"Machine Learning: Methods and Applications to Brain Disorders",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089910901&doi=10.1016%2fC2017-0-03724-2&partnerID=40&md5=54ea263a08fbead72e045ac0eeaf745f","Machine Learning is an area of artificial intelligence involving the development of algorithms to discover trends and patterns in existing data; this information can then be used to make predictions on new data. A growing number of researchers and clinicians are using machine learning methods to develop and validate tools for assisting the diagnosis and treatment of patients with brain disorders. Machine Learning: Methods and Applications to Brain Disorders provides an up-to-date overview of how these methods can be applied to brain disorders, including both psychiatric and neurological disease. This book is written for a non-technical audience, such as neuroscientists, psychologists, psychiatrists, neurologists and health care practitioners. © 2020 Elsevier Inc. All rights reserved.",Book,"Final",Scopus,2-s2.0-85089910901
"Ali M., Ullah I., Noor W., Sajid A., Basit A., Baber J.","Predicting the session of an P2P IPTV user through support vector regression (SVR)",2020,"Engineering, Technology & Applied Science Research",2,,[No abstract available],,"Final",Scopus,2-s2.0-85097771566
"Smith S.O.M., Susarla D.",[No title available],2018,"Feature Engineering Made Easy",1,,[No abstract available],,"Final",Scopus,2-s2.0-85097781645
"Xiao S., Nie M.","The preventive maintenance of highway based on data mining",2017,"MATEC Web of Conferences",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039415733&doi=10.1051%2fmatecconf%2f201713900084&partnerID=40&md5=07ea9304f0c30a49465d4e677e991a8d","Judging from the current situation of Chinese highway maintenance,only after there is highway distress would the staff have a repair, that results in the poor efficiency of highway maintenance. In order to improve the efficiency of highway maintenance,this paper will use data mining technology to predict the pavement performance of highway and analyze the main factors of pavement performance attenuation,so that the preventive maintenance can be carried out.We will provide data support to the preventive maintenance of highway by using the isolation Forest anomaly detection algorithm to have a data pretreatment, the regression model and time series GM (1,1) model to predict the pavement performance and the association rule analysis and isolation Forest to analyze the main factors of pavement performance attenuation. © The Authors, published by EDP Sciences, 2017.",Conference Paper,"Final",Scopus,2-s2.0-85039415733
